=====./preprocessors/column_selector.py=====
import pandas as pd
from typing import List, Dict

class ColumnSelector:
    def __init__(self, data: pd.DataFrame, num_classes: int = 5):
        self.data = data
        self.num_classes = num_classes  # Max number of classes for a column to be considered nominal

    def get_numerical_columns(self) -> List[str]:
        numerical_columns = self.data.select_dtypes(include=['int64', 'float64']).columns.tolist()
        return numerical_columns if numerical_columns else None

    def get_nominal_columns(self) -> List[str]:
        condition = lambda col: (
            (self.data[col].dtype == 'object' or self.data[col].dtype == 'int64') and
            self.data[col].nunique() < self.num_classes
        )
        nominal_columns = [col for col in self.data.columns if condition(col)]
        return nominal_columns if nominal_columns else None
    
    def get_ordinal_columns(self) -> List[str]:
        # Assumes ordinal columns have "ordinal" in their name or are integers
        condition = lambda col: (
            (isinstance(col, int) or 'ordinal' in str(col)) and
            self.data[col].nunique() <= self.num_classes
        )
        ordinal_columns = [col for col in self.data.columns if condition(col)]
        return ordinal_columns if ordinal_columns else None

    def get_ordinal_categories(self) -> Dict[str, List]:
        ordinal_columns = self.get_ordinal_columns()
        if not ordinal_columns:
            return None
        ordinal_categories = {col: self.data[col].unique().tolist() for col in ordinal_columns}
        return ordinal_categories if ordinal_categories else None

    def get_columns_by_regex(self, regex_pattern: str) -> List[str]:
        columns_by_regex = self.data.filter(regex=regex_pattern).columns.tolist()
        return columns_by_regex if columns_by_regex else None
=====./preprocessors/data_loader.py=====
import pandas as pd

class DataLoader():
    """
    Class to load data from a CSV file in a dataframe.
    """

    @staticmethod
    def load_data(file_path: str, delimiter: str = ',', encoding: str = 'utf-8') -> None:
        """
        Load data from the CSV file into a DataFrame.
        """
        return pd.read_csv(file_path, delimiter=delimiter, encoding=encoding)
=====./preprocessors/data_encoder.py=====
import pandas as pd
from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder, MinMaxScaler, OrdinalEncoder
from sklearn.compose import ColumnTransformer
from core.preprocessors.column_selector import ColumnSelector

class DataEncoder():
    def __init__(self, num_classes: int, scaling_strategy: str = 'standard', select_numerical: bool = False, select_nominal: bool = False, select_ordinal: bool = False):
        self.num_classes = num_classes
        self.scaling_strategy = scaling_strategy
        self.column_selector = None
        self.column_transformer = None
        self.numerical_columns = None
        self.nominal_columns = None
        self.ordinal_columns = None
        self.ordinal_categories = None
        self.select_numerical = select_numerical
        self.select_nominal = select_nominal
        self.select_ordinal = select_ordinal

    def initialize_encoder(self):
        transformers = []

        if self.numerical_columns is not None:
            if self.scaling_strategy == 'standard':
                transformers.append(('num_standard', StandardScaler(), self.numerical_columns))
            elif self.scaling_strategy == 'minmax':
                transformers.append(('num_minmax', MinMaxScaler(), self.numerical_columns))
            elif self.scaling_strategy == 'both':
                transformers.append(('num_standard', StandardScaler(), self.numerical_columns))
                transformers.append(('num_minmax', MinMaxScaler(), self.numerical_columns))

        if self.nominal_columns is not None:
            transformers.append(('nom', OneHotEncoder(sparse=False, handle_unknown='ignore'), self.nominal_columns))

        if self.ordinal_columns is not None:
            categories = [self.ordinal_categories[col] for col in self.ordinal_columns]
            transformers.append(('ord', OrdinalEncoder(categories=categories), self.ordinal_columns))

        self.column_transformer = ColumnTransformer(transformers=transformers)

    def select_columns(self, X: pd.DataFrame):
        """ Seleciona colunas numéricas, nominais e ordinais
        baseado em algumas heurísticas genéricas definidas em ColumnSelector.
    
        Args:
            X (pd.DataFrame): O DataFrame de entrada.
            select_numerical (bool): Se True, seleciona colunas numéricas.
            select_nominal (bool): Se True, seleciona colunas nominais.
            select_ordinal (bool): Se True, seleciona colunas ordinais.
        """
        self.column_selector = ColumnSelector(X, self.num_classes)
        
        if self.select_numerical:
            self.numerical_columns = self.column_selector.get_numerical_columns()
        else:
            self.numerical_columns = None
    
        if self.select_nominal:
            self.nominal_columns = self.column_selector.get_nominal_columns()
        else:
            self.nominal_columns = None
    
        if self.select_ordinal:
            self.ordinal_columns = self.column_selector.get_ordinal_columns()
            self.ordinal_categories = self.column_selector.get_ordinal_categories()
        else:
            self.ordinal_columns = None
            self.ordinal_categories = None

    def fit(self, X: pd.DataFrame, y=None):
        self.select_columns(X)
        self.initialize_encoder()
        self.column_transformer.fit(X)
        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        X_transformed = self.column_transformer.transform(X)
        feature_names = self.column_transformer.get_feature_names_out()
        if X_transformed.shape[1] != len(feature_names):
            raise ValueError(f"DataEncoder: Shape of transformed data is {X_transformed.shape}, but got {len(feature_names)} feature names.")
        return pd.DataFrame(X_transformed, columns=feature_names, index=X.index)

    def fit_transform(self, X: pd.DataFrame, y=None) -> pd.DataFrame:
        self.fit(X, y)
        return self.transform(X)
    
    @staticmethod
    def encode_y(y):
        y_encoded = LabelEncoder().fit_transform(y)
        return y_encoded=====./preprocessors/data_balancer.py=====
import pandas as pd
from imblearn.over_sampling import SMOTE
from typing import Tuple

class DataBalancer:
    def __init__(self, random_state: int = 42):
        self.random_state = random_state

    def apply_smote(self, X: pd.DataFrame, y: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:
        smote = SMOTE(random_state=self.random_state)
        X_resampled, y_resampled = smote.fit_resample(X, y)
        
        # Verifique se y é um pandas.Series ou numpy.ndarray
        if isinstance(y, pd.Series):
            y_name = y.name
        else:
            y_name = "target"
        
        return pd.DataFrame(X_resampled, columns=X.columns), pd.Series(y_resampled, name=y_name)=====./preprocessors/data_cleaner.py=====
class DataCleaner():
    
    @staticmethod
    def remove_instances_with_value(data, column: str, value: str):
        """
        Remove instances where the specified column has the specified value.

        Args:
            data (pd.DataFrame): The input data.
            column (str): The column to check.
            value (str): The value to remove.

        Returns:
            pd.DataFrame: The cleaned data.
        """
        return data[data[column] != value]
    
    @staticmethod
    def remove_columns(data, columns: list):
        """
        Remove the specified columns from the data.

        Args:
            data (pd.DataFrame): The input data.
            columns (list): The columns to remove.

        Returns:
            pd.DataFrame: The cleaned data.
        """
        return data.drop(columns=columns)
    =====./preprocessors/data_splitter.py=====
from typing import Tuple
import pandas as pd
from sklearn.model_selection import train_test_split, StratifiedShuffleSplit

class DataSplitter:
    @staticmethod
    def split_by_student_level(data, test_size=0.2, column_name='aluno'):
        unique_students = data[column_name].unique()
        train_students, test_students = train_test_split(unique_students, test_size=test_size, random_state=42)
        train_data = data[data[column_name].isin(train_students)]
        test_data = data[data[column_name].isin(test_students)]
        return train_data, test_data

    @staticmethod
    def split_by_stratified_student_level(data, test_size=0.2, column_name='aluno', target_column='comportamento', n_splits=10):
        unique_students = data[column_name].unique()
        stratified_split = StratifiedShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=42)
        y = data.groupby(column_name)[target_column].first().loc[unique_students]
        for train_index, test_index in stratified_split.split(unique_students, y):
            train_students = unique_students[train_index]
            test_students = unique_students[test_index]
            break
        train_data = data[data[column_name].isin(train_students)]
        test_data = data[data[column_name].isin(test_students)]
        return train_data, test_data

    @staticmethod
    def split_data_stratified(data, test_size=0.2, target_column='comportamento', n_splits=1, random_state=42):
        stratified_split = StratifiedShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=random_state)
        for train_index, test_index in stratified_split.split(data, data[target_column]):
            train_data = data.iloc[train_index]
            test_data = data.iloc[test_index]
            break
        return train_data, test_data

    @staticmethod
    def split_into_x_y(data: pd.DataFrame, target_column: str) -> Tuple[pd.DataFrame, pd.Series]:
        X = data.drop(columns=[target_column])
        y = data[target_column]
        return X, y=====./training/model_params.py=====
from abc import ABC, abstractmethod

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier
import xgboost as xgb

class ModelParams(ABC):

    @staticmethod
    def get_models():
        return {
            'Logistic Regression': LogisticRegression(max_iter=5000),
            'Decision Tree': DecisionTreeClassifier(),
            'Random Forest': RandomForestClassifier(),
            'Gradient Boosting': GradientBoostingClassifier(),
            'SVM': SVC(probability=True),
            'KNN': KNeighborsClassifier(),
            'XGBoost': xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
            # 'Naive Bayes': GaussianNB(),
            # 'MLP': MLPClassifier()
        }

    @staticmethod
    @abstractmethod
    def get_param_space(model_name):
        pass=====./training/skopt_model_params.py=====
from skopt.space import Real, Integer, Categorical
from core.training.model_params import ModelParams

class SkoptModelParams(ModelParams):

    @staticmethod
    def get_bayes_search_spaces():
        return {
            'Logistic Regression': SkoptModelParams._get_logistic_regression_space(),
            'Decision Tree': SkoptModelParams._get_decision_tree_space(),
            'Random Forest': SkoptModelParams._get_random_forest_space(),
            'Gradient Boosting': SkoptModelParams._get_gradient_boosting_space(),
            'SVM': SkoptModelParams._get_svm_space(),
            'KNN': SkoptModelParams._get_knn_space(),
            'XGBoost': SkoptModelParams._get_xgboost_space()
        }

    @staticmethod
    def _get_logistic_regression_space():
        return [
            {
                'classifier__penalty': Categorical(['l2']),
                'classifier__C': Real(0.01, 10, prior='log-uniform'),
                'classifier__solver': Categorical(['lbfgs', 'saga']),
                'classifier__max_iter': Integer(1000, 10000)
            },
            {
                'classifier__penalty': Categorical(['l1']),
                'classifier__C': Real(0.01, 10, prior='log-uniform'),
                'classifier__solver': Categorical(['liblinear', 'saga']),
                'classifier__max_iter': Integer(1000, 10000)
            }
        ]

    @staticmethod
    def _get_decision_tree_space():
        return {
            'classifier__max_depth': Categorical([None, 3, 5, 10, 20, 30]),
            'classifier__min_samples_split': Integer(2, 20),
            'classifier__min_samples_leaf': Integer(1, 10)
        }

    @staticmethod
    def _get_random_forest_space():
        return {
            'classifier__n_estimators': Integer(50, 300),
            'classifier__max_depth': Integer(3, 30),
            'classifier__min_samples_split': Integer(2, 20),
            'classifier__min_samples_leaf': Integer(1, 10),
            'classifier__max_features': Categorical(['sqrt', 'log2', None])
        }

    @staticmethod
    def _get_gradient_boosting_space():
        return {
            'classifier__n_estimators': Integer(50, 300),
            'classifier__learning_rate': Real(0.01, 0.2, prior='uniform'),
            'classifier__max_depth': Integer(3, 10),
            'classifier__subsample': Real(0.5, 1.0, prior='uniform'),
            'classifier__min_samples_split': Integer(2, 20),
            'classifier__min_samples_leaf': Integer(1, 10)
        }

    @staticmethod
    def _get_svm_space():
        return [
            {
                'classifier__C': Real(0.01, 10, prior='log-uniform'),
                'classifier__kernel': Categorical(['rbf']),
                'classifier__gamma': Real(1e-4, 1e-1, prior='log-uniform')
            },
            {
                'classifier__C': Real(0.01, 10, prior='log-uniform'),
                'classifier__kernel': Categorical(['linear']),
                'classifier__gamma': Categorical(['scale'])
            },
            {
                'classifier__C': Real(0.01, 10, prior='log-uniform'),
                'classifier__kernel': Categorical(['poly']),
                'classifier__gamma': Real(1e-4, 1e-1, prior='log-uniform'),
                'classifier__degree': Integer(2, 5)
            },
            {
                'classifier__C': Real(0.01, 10, prior='log-uniform'),
                'classifier__kernel': Categorical(['sigmoid']),
                'classifier__gamma': Real(1e-4, 1e-1, prior='log-uniform')
            }
        ]

    @staticmethod
    def _get_knn_space():
        return {
            'classifier__n_neighbors': Integer(3, 20),
            'classifier__weights': Categorical(['uniform', 'distance']),
            'classifier__metric': Categorical(['euclidean', 'manhattan', 'minkowski'])
        }

    @staticmethod
    def _get_xgboost_space():
        return {
            'classifier__n_estimators': Integer(50, 300),
            'classifier__learning_rate': Real(0.01, 0.2, prior='uniform'),
            'classifier__max_depth': Integer(3, 10),
            'classifier__subsample': Real(0.7, 1.0, prior='uniform'),
            'classifier__colsample_bytree': Real(0.7, 1.0, prior='uniform'),
            'classifier__reg_alpha': Real(0.0, 1.0, prior='uniform'),
            'classifier__reg_lambda': Real(0.0, 1.0, prior='uniform')
        }=====./training/grid_search_training.py=====
import pandas as pd
from sklearn.model_selection import GridSearchCV
from core.logging.logger_config import LoggerConfig
from core.training.model_training import ModelTraining
from core.training.grid_search_model_params import GridSearchModelParams
import logging


class GridSearchTraining(ModelTraining):
    def __init__(self):
        super().__init__(logger_name='grid_search_training')

    def optimize_model(self, pipeline, model_name, selector_name, X_train, y_train, n_iter, cv, scoring, n_jobs=-1, selector_search_space=None):
        self.logger.info(f"Training and evaluating {model_name} with GridSearchCV and {selector_name}")

        # Combine model and selector parameter grids
        param_grid = GridSearchModelParams.get_param_grid(model_name)
        if selector_search_space:
            if isinstance(param_grid, list):
                for subspace in param_grid:
                    subspace.update(selector_search_space)
            else:
                param_grid.update(selector_search_space)

        self.logger.debug(f"Parameter grid: {param_grid}")

        grid_search = GridSearchCV(
            estimator=pipeline,
            param_grid=param_grid,
            cv=cv,
            n_jobs=n_jobs,
            scoring=scoring,
            verbose=1
        )

        self.logger.info("Starting GridSearchCV fitting process")
        grid_search.fit(X_train, y_train)
        self.logger.info("GridSearchCV fitting process completed")

        self.log_search_results(grid_search)

        # Store the results
        self.trained_models[f"{model_name}_{selector_name}"] = {
            'model': grid_search.best_estimator_,
            'training_type': "GridSearchCV",
            'hyperparameters': grid_search.best_params_,
            'cv_result': grid_search.best_score_
        }
        self.logger.info(f"Model {model_name}_{selector_name} stored successfully")

    # def _log_grid_search_results(self, grid_search, model_name, selector_name):
    #     """Log the results of the GridSearchCV process."""
    #     self.logger.info(f"Best parameters: {grid_search.best_params_}")
    #     self.logger.info(f"Best cross-validation score: {grid_search.best_score_}")

    #     # Log all hyperparameter combinations and their cross-validation results
    #     self.logger.info("All hyperparameter combinations and their cross-validation results:")
    #     cv_results = grid_search.cv_results_
    #     nan_count = 0
    #     for mean_score, params in zip(cv_results['mean_test_score'], cv_results['params']):
    #         if pd.isna(mean_score):
    #             nan_count += 1
    #         self.logger.info(f"Params: {params}, Mean Test Score: {mean_score}")

    #     self.logger.info(f"Number of tests that resulted in NaN for {model_name}: {nan_count}")=====./training/model_training.py=====
from abc import ABC, abstractmethod
from sklearn.pipeline import Pipeline

from typing import List, Optional, Dict, Any
import logging
import pandas as pd

from core.logging.logger_config import LoggerConfig
from core.training.model_params import ModelParams
from core.feature_selection.feature_selection_factory import FeatureSelectionFactory


class ModelTraining(ABC):
    def __init__(self, logger_name):
        # Configurar um logger nomeado
        LoggerConfig.configure_log_file(
            file_main_name=logger_name,
            log_extension=".log",
            logger_name=logger_name
        )
        self.logger = logging.getLogger(logger_name)
        self.trained_models: Dict[str, Any] = {}

    def train_model(
        self,
        X_train,
        y_train,
        selected_models: Optional[List[str]] = None, # Lista de nomes de modelos a serem treinados.Se None, todos os modelos disponíveis serão utilizados.
        selected_selectors: Optional[List[str]] = None, # Lista de nomes de seletores a serem utilizados. Se None, todos os seletores disponíveis serão utilizados.
        n_iter: int = 50, # Número de iterações para otimização.
        cv: int = 5, # Número de folds para validação cruzada.
        scoring: str = 'balanced_accuracy', # Métrica de avaliação.
        n_jobs: int = -1 # Número de trabalhos paralelos.
    ) -> Dict[str, Any]: # Dicionário contendo os modelos treinados e seus resultados.
        """
        Treina modelos com diferentes seletores de características.
        """
        models = ModelParams.get_models()
        available_selector_names = FeatureSelectionFactory.get_available_selectors()

        # Filtrar modelos
        models = self._filter_models(models, selected_models)

        # Filtrar seletores
        selector_names = self._filter_selectors(selected_selectors, available_selector_names)

        for model_name, model_config in models.items():
            for selector_name in selector_names:
                # Criar a instância do seletor diretamente dentro do loop
                selector_instance = FeatureSelectionFactory.create_selector(selector_name, X_train, y_train)
                selector = selector_instance.selector  # Acessar o seletor criado no construtor

                pipeline = self._create_pipeline(selector, model_config)

                # Obter o espaço de busca diretamente do selector_instance
                selector_search_space = selector_instance.get_search_space()

                self.optimize_model(
                    pipeline,
                    model_name,
                    selector_name,
                    X_train,
                    y_train,
                    n_iter,
                    cv,
                    scoring,
                    n_jobs,
                    selector_search_space  # Passar o espaço de busca
                )

        return self.trained_models

    @abstractmethod
    def optimize_model(
        self,
        pipeline,
        model_name: str,
        selector_name: str,
        X_train,
        y_train,
        n_iter: int,
        cv: int,
        scoring: str,
        n_jobs: int,
        selector_search_space: dict
    ):
        pass

    @staticmethod
    def _create_pipeline(selector, model_config) -> Pipeline:
        """
        Cria um pipeline com seleção de características e o classificador.

        Args:
            selector: Seletor de características.
            model_config: Configuração do modelo de classificação.

        Returns:
            Pipeline: Pipeline configurado.
        """
        return Pipeline([
            ('feature_selection', selector),
            ('classifier', model_config)
        ])

    def _filter_models(self, models: Dict[str, Any], selected_models: Optional[List[str]]) -> Dict[str, Any]:
        """
        Filtra os modelos baseados na lista de modelos selecionados.

        Args:
            models (Dict[str, Any]): Dicionário de modelos disponíveis.
            selected_models (Optional[List[str]]): Lista de modelos a serem utilizados.

        Returns:
            Dict[str, Any]: Dicionário filtrado de modelos.

        Raises:
            ValueError: Se algum modelo selecionado não for encontrado.
        """
        if selected_models is not None:
            filtered_models = {name: cfg for name, cfg in models.items() if name in selected_models}
            missing_models = set(selected_models) - set(filtered_models.keys())
            if missing_models:
                raise ValueError(f"Modelos não encontrados: {missing_models}")
            return filtered_models
        return models

    def _filter_selectors(self, selected_selectors: Optional[List[str]], available_selector_names: List[str]) -> List[str]:
        """
        Filtra os seletores baseados na lista de seletores selecionados.

        Args:
            selected_selectors (Optional[List[str]]): Lista de seletores a serem utilizados.
            available_selector_names (List[str]): Lista de seletores disponíveis.

        Returns:
            List[str]: Lista filtrada de seletores.

        Raises:
            ValueError: Se algum seletor selecionado não for encontrado.
        """
        if selected_selectors is not None:
            selector_names = [s for s in selected_selectors if s in available_selector_names]
            missing_selectors = set(selected_selectors) - set(selector_names)
            if missing_selectors:
                raise ValueError(f"Seletores não encontrados: {missing_selectors}")
            return selector_names
        return available_selector_names
    
    @staticmethod
    def log_search_results(logger, search, model_name, selector_name):
        """Log the results of the search process (GridSearchCV, RandomizedSearchCV, or any search with cv_results_)."""
        if hasattr(search, 'best_params_'):
            logger.info(f"Best parameters: {search.best_params_}")
        if hasattr(search, 'best_score_'):
            logger.info(f"Best cross-validation score: {search.best_score_}")

        # Log all hyperparameter combinations and their cross-validation results
        logger.info("All hyperparameter combinations and their cross-validation results:")
        if hasattr(search, 'cv_results_'):
            cv_results = search.cv_results_
            nan_count = 0
            for mean_score, params in zip(cv_results['mean_test_score'], cv_results['params']):
                if pd.isna(mean_score):
                    nan_count += 1
                logger.info(f"Params: {params}, Mean Test Score: {mean_score}")
            logger.info(f"Number of tests that resulted in NaN for {model_name}: {nan_count}")
=====./training/optuna_model_params.py=====
from core.training.model_params import ModelParams

class OptunaModelParams(ModelParams):


    @staticmethod
    def suggest_hyperparameters(trial, model_name):
        """
        Sugere hiperparâmetros para o modelo especificado usando um trial do Optuna.
        """
        model_methods = {
            'Logistic Regression': OptunaModelParams._suggest_logistic_regression,
            'Decision Tree': OptunaModelParams._suggest_decision_tree,
            'Random Forest': OptunaModelParams._suggest_random_forest,
            'Gradient Boosting': OptunaModelParams._suggest_gradient_boosting,
            'SVM': OptunaModelParams._suggest_svm,
            'KNN': OptunaModelParams._suggest_knn,
            'XGBoost': OptunaModelParams._suggest_xgboost
            # 'Naive Bayes': OptunaModelParams._suggest_naive_bayes(trial),
            # 'MLP': OptunaModelParams._suggest_mlp(trial)
        }
        
        if model_name not in model_methods:
            raise ValueError(f"Modelo desconhecido: {model_name}")
        
        return model_methods[model_name](trial)

    @staticmethod
    def _suggest_logistic_regression(trial):
        penalty = trial.suggest_categorical('classifier__penalty', ['l1', 'l2'])
        C = trial.suggest_float('classifier__C', 0.01, 10, log=True)
        
        if penalty == 'l1':
            solver = trial.suggest_categorical('classifier__solver', ['liblinear', 'saga'])
        else:  # l2
            solver = trial.suggest_categorical('classifier__solver', ['lbfgs', 'newton-cg', 'sag', 'saga'])
        
        max_iter = trial.suggest_int('classifier__max_iter', 1000, 10000)
        
        return {
            'classifier__penalty': penalty,
            'classifier__C': C,
            'classifier__solver': solver,
            'classifier__max_iter': max_iter
        }

    @staticmethod
    def _suggest_decision_tree(trial):
        return {
            'classifier__max_depth': trial.suggest_categorical('classifier__max_depth', [None, 3, 5, 10, 20, 30]),
            'classifier__min_samples_split': trial.suggest_int('classifier__min_samples_split', 2, 20),
            'classifier__min_samples_leaf': trial.suggest_int('classifier__min_samples_leaf', 1, 10)
        }

    @staticmethod
    def _suggest_random_forest(trial):
        return {
            'classifier__n_estimators': trial.suggest_int('classifier__n_estimators', 50, 300, step=50),
            'classifier__max_depth': trial.suggest_int('classifier__max_depth', 3, 30),
            'classifier__min_samples_split': trial.suggest_int('classifier__min_samples_split', 2, 20),
            'classifier__min_samples_leaf': trial.suggest_int('classifier__min_samples_leaf', 1, 10),
            'classifier__max_features': trial.suggest_categorical('classifier__max_features', ['sqrt', 'log2', None])
        }

    @staticmethod
    def _suggest_gradient_boosting(trial):
        return {
            'classifier__n_estimators': trial.suggest_int('classifier__n_estimators', 50, 300, step=50),
            'classifier__learning_rate': trial.suggest_float('classifier__learning_rate', 0.01, 0.2),
            'classifier__max_depth': trial.suggest_int('classifier__max_depth', 3, 10),
            'classifier__subsample': trial.suggest_float('classifier__subsample', 0.5, 1.0),
            'classifier__min_samples_split': trial.suggest_int('classifier__min_samples_split', 2, 20),
            'classifier__min_samples_leaf': trial.suggest_int('classifier__min_samples_leaf', 1, 10)
        }

    @staticmethod
    def _suggest_svm(trial):
        params = {
            'classifier__C': trial.suggest_float('classifier__C', 0.01, 10, log=True),
            'classifier__kernel': trial.suggest_categorical('classifier__kernel', ['rbf', 'linear', 'poly', 'sigmoid'])
        }
        if params['classifier__kernel'] in ['rbf', 'poly', 'sigmoid']:
            params['classifier__gamma'] = trial.suggest_float('classifier__gamma', 1e-4, 1e-1, log=True)
        if params['classifier__kernel'] == 'poly':
            params['classifier__degree'] = trial.suggest_int('classifier__degree', 2, 5)
        return params

    @staticmethod
    def _suggest_knn(trial):
        return {
            'classifier__n_neighbors': trial.suggest_int('classifier__n_neighbors', 3, 20),
            'classifier__weights': trial.suggest_categorical('classifier__weights', ['uniform', 'distance']),
            'classifier__metric': trial.suggest_categorical('classifier__metric', ['euclidean', 'manhattan', 'minkowski'])
        }

    @staticmethod
    def _suggest_xgboost(trial):
        return {
            'classifier__n_estimators': trial.suggest_int('classifier__n_estimators', 50, 300, step=50),
            'classifier__learning_rate': trial.suggest_float('classifier__learning_rate', 0.01, 0.2),
            'classifier__max_depth': trial.suggest_int('classifier__max_depth', 3, 10),
            'classifier__subsample': trial.suggest_float('classifier__subsample', 0.7, 1.0),
            'classifier__colsample_bytree': trial.suggest_float('classifier__colsample_bytree', 0.7, 1.0),
            'classifier__reg_alpha': trial.suggest_float('classifier__reg_alpha', 0.0, 1.0),
            'classifier__reg_lambda': trial.suggest_float('classifier__reg_lambda', 0.0, 1.0)
        }=====./training/optuna_bayesian_optimization_training.py=====
import optuna
from optuna.samplers import TPESampler
from sklearn.model_selection import cross_val_score
from core.training.model_training import ModelTraining
from core.training.optuna_model_params import OptunaModelParams
from core.logging.logger_config import LoggerConfig
import logging
from time import time
import pandas as pd

class OptunaBayesianOptimizationTraining(ModelTraining):
    def __init__(self):
        super().__init__(logger_name='optuna_training')

    def optimize_model(self, pipeline, model_name, selector_name, X_train, y_train, n_trials, cv, scoring, n_jobs=-1, selector_search_space=None):
        self.logger.info(f"Training and evaluating {model_name} with Optuna Bayesian Optimization and {selector_name}")
        print(f"Training and evaluating {model_name} with Optuna Bayesian Optimization and {selector_name}")

        def objective(trial):
            try:
                model_hyperparams = OptunaModelParams.suggest_hyperparameters(trial, model_name)
                
                # Sugerir hiperparâmetros para o seletor de features
                selector_hyperparams = {}
                for param, values in selector_search_space.items():
                    if isinstance(values, list) and isinstance(values[0], int):
                        selector_hyperparams[param] = trial.suggest_int(param, min(values), max(values))
                    elif isinstance(values, list) and isinstance(values[0], float):
                        selector_hyperparams[param] = trial.suggest_float(param, min(values), max(values))
                    elif isinstance(values, list) and isinstance(values[0], str):
                        selector_hyperparams[param] = trial.suggest_categorical(param, values)
                    else:
                        selector_hyperparams[param] = trial.suggest_categorical(param, values)
                
                # Combinar os hiperparâmetros do modelo e do seletor
                hyperparams = {**model_hyperparams, **selector_hyperparams}
                
                pipeline.set_params(**hyperparams)
                
                score = cross_val_score(
                    estimator=pipeline,
                    X=X_train,
                    y=y_train,
                    scoring=scoring,
                    cv=cv,
                    n_jobs=n_jobs
                ).mean()
                
                return score
            except Exception as e:
                self.logger.warning(f"Trial failed with error: {str(e)}")
                return float('-inf')  # Retorna um valor muito baixo para indicar que o trial falhou
        
        # Criar um estudo do Optuna
        study = optuna.create_study(direction='maximize', sampler=TPESampler())
        
        # Iniciar o tempo de otimização
        start_time = time()
        study.optimize(objective, n_trials=n_trials)
        total_time = time() - start_time

        # Atualizar o pipeline com os melhores hiperparâmetros
        pipeline.set_params(**study.best_trial.params)
        
        # Treinar o modelo final com os melhores hiperparâmetros
        pipeline.fit(X_train, y_train)

        # Log the results using the overridden method
        self.log_search_results(self.logger, study, model_name, selector_name)
        
        # Armazenar os resultados
        self.trained_models[f"{model_name}_{selector_name}"] = {
            'model': pipeline,
            'training_type': "Optuna Bayesian Optimization",
            'hyperparameters': study.best_trial.params,
            'cv_result': study.best_trial.value,
            'optimization_time_seconds': total_time
        }

        # Log final do melhor resultado
        self.logger.info(f"Optuna Optimization Best Result for {model_name} with {selector_name}: {study.best_trial.value}")
        print(f"Optuna Optimization Best Result for {model_name} with {selector_name}: {study.best_trial.value}")
    
    @staticmethod
    def log_search_results(logger, study, model_name, selector_name):
        """Log the results of the Optuna optimization process."""
        logger.info(f"Best parameters: {study.best_params}")
        logger.info(f"Best cross-validation score: {study.best_value}")

        # Log all hyperparameter combinations and their cross-validation results
        logger.info("All hyperparameter combinations and their cross-validation results:")
        nan_count = 0
        for trial in study.trials:
            mean_score = trial.value
            params = trial.params
            if pd.isna(mean_score):
                nan_count += 1
            logger.info(f"Params: {params}, Mean Test Score: {mean_score}")
        logger.info(f"Number of tests that resulted in NaN for {model_name}: {nan_count}")
=====./training/random_search_training.py=====
from sklearn.model_selection import RandomizedSearchCV
from core.training.model_training import ModelTraining
from core.training.grid_search_model_params import GridSearchModelParams

class RandomSearchTraining(ModelTraining):
    def __init__(self):
        super().__init__(logger_name='random_search_training')

    def optimize_model(self, pipeline, model_name, selector_name, X_train, y_train, n_iter, cv, scoring, n_jobs=-1, selector_search_space=None):
        self.logger.info(f"Training and evaluating {model_name} with RandomizedSearchCV and {selector_name}")

        # Combine model and selector parameter grids
        param_grid = GridSearchModelParams.get_param_grid(model_name)
        if selector_search_space:
            if isinstance(param_grid, list):
                for subspace in param_grid:
                    subspace.update(selector_search_space)
            else:
                param_grid.update(selector_search_space)

        self.logger.debug(f"Parameter grid: {param_grid}")

        random_search = RandomizedSearchCV(
            estimator=pipeline,
            param_distributions=param_grid,
            n_iter=n_iter,
            cv=cv,
            n_jobs=n_jobs,
            scoring=scoring,
            verbose=1,
            random_state=42  # Para reprodutibilidade
        )

        self.logger.info("Starting RandomizedSearchCV fitting process")
        random_search.fit(X_train, y_train)
        self.logger.info("RandomizedSearchCV fitting process completed")

        # Log the results using ModelTraining's method
        self.log_search_results(self.logger, random_search, model_name, selector_name)

        self.logger.info(f"Best parameters: {random_search.best_params_}")
        self.logger.info(f"Best cross-validation score: {random_search.best_score_}")

        # Store the results
        self.trained_models[f"{model_name}_{selector_name}"] = {
            'model': random_search.best_estimator_,
            'training_type': "RandomizedSearchCV",
            'hyperparameters': random_search.best_params_,
            'cv_result': random_search.best_score_
        }
        self.logger.info(f"Model {model_name}_{selector_name} stored successfully")=====./training/grid_search_model_params.py=====
from core.training.model_params import ModelParams


class GridSearchModelParams(ModelParams):
    @staticmethod
    def get_param_grid(model_name):
        param_grid_methods = {
            'Logistic Regression': GridSearchModelParams._get_logistic_regression_params,
            'Random Forest': GridSearchModelParams._get_random_forest_params,
            'Gradient Boosting': GridSearchModelParams._get_gradient_boosting_params,
            'SVM': GridSearchModelParams._get_svm_params,
            'XGBoost': GridSearchModelParams._get_xgboost_params
        }
        return param_grid_methods.get(model_name, lambda: {})()

    @staticmethod
    def _get_logistic_regression_params():
        return [
            {
                'classifier__penalty': ['l1'],
                'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],
                'classifier__solver': ['liblinear', 'saga']
            },
            {
                'classifier__penalty': ['l2'],
                'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],
                'classifier__solver': ['lbfgs', 'newton-cg', 'sag', 'saga']
            },
            {
                'classifier__penalty': ['elasticnet'],
                'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],
                'classifier__solver': ['saga'],
                'classifier__l1_ratio': [0.25, 0.5, 0.75]
            },
            {
                'classifier__penalty': ['none'],
                'classifier__solver': ['lbfgs', 'newton-cg', 'sag', 'saga']
            }
        ]

    @staticmethod
    def _get_random_forest_params():
        return {
            'classifier__n_estimators': [50, 100, 200],
            'classifier__max_depth': [None, 10, 20, 30],
            'classifier__min_samples_split': [2, 5, 10],
            'classifier__min_samples_leaf': [1, 2, 4]
        }

    @staticmethod
    def _get_gradient_boosting_params():
        return {
            'classifier__n_estimators': [50, 100, 200],
            'classifier__learning_rate': [0.01, 0.1, 0.2],
            'classifier__max_depth': [3, 4, 5]
        }

    @staticmethod
    def _get_svm_params():
        return {
            'classifier__C': [0.1, 1, 10],
            'classifier__kernel': ['rbf', 'linear'],
            'classifier__gamma': ['scale', 'auto']
        }

    @staticmethod
    def _get_xgboost_params():
        return {
            'classifier__n_estimators': [50, 100, 200],
            'classifier__learning_rate': [0.01, 0.1, 0.3],
            'classifier__max_depth': [3, 4, 5]
        }
    
=====./training/skopt_bayesian_optimization_training.py=====
# bayesian_optimization_training.py

from skopt import BayesSearchCV
from core.training.model_training import ModelTraining
from core.training.skopt_model_params import SkoptModelParams
from core.logging.logger_config import LoggerConfig
import logging

class SkoptBayesianOptimizationTraining(ModelTraining):
    def __init__(self):
        super().__init__(logger_name='skopt_training')

    def optimize_model(self, pipeline, model_name, selector_name, X_train, y_train, n_iter, cv, scoring, n_jobs=-1, selector_search_space=None):
        logging.info(f"Training and evaluating {model_name} with Bayesian Optimization and {selector_name}")
        print(f"Training and evaluating {model_name} with Bayesian Optimization and {selector_name}")

        # Obter o espaço de busca específico do modelo
        search_space_model = SkoptModelParams.get_bayes_search_spaces().get(model_name, {})
        # Obter o espaço de busca específico do seletor (já passado como parâmetro)
        search_space_selector = selector_search_space

        # Combinar os espaços de busca do modelo e do seletor
        if isinstance(search_space_model, list):
            for subspace in search_space_model:
                subspace.update(search_space_selector)
        else:
            search_space_model.update(search_space_selector)

        logging.info(f"Search space for {model_name} with selector {selector_name}: {search_space_model}")

        best_model, best_result, opt = self._execute_bayesian_optimization(
            pipeline,
            search_space_model,
            X_train,
            y_train,
            n_iter,
            cv,
            scoring,
            n_jobs
        )

        # Log the results using ModelTraining's method
        self.log_search_results(self.logger, opt, model_name, selector_name)

        logging.info(f"Bayesian optimization results: Best result: {best_result}, "
                     f"Average cross-validation result: {opt.cv_results_['mean_test_score'][opt.best_index_]}")
        
        self._store_model_results(model_name, selector_name, best_model, best_result)

    def _execute_bayesian_optimization(self, pipeline, space, X_train, y_train, n_iter=50, cv=5, scoring='balanced_accuracy', n_jobs=-1):
        search = BayesSearchCV(
            pipeline,
            search_spaces=space,
            n_iter=n_iter,
            cv=cv,
            scoring=scoring,
            n_jobs=n_jobs,
            random_state=42,
            verbose=3
        )

        search.fit(X_train, y_train, callback=LoggerConfig.log_results)
        return search.best_estimator_, search.best_score_, search

    def _store_model_results(self, model_name, selector_name, best_model, best_result):
        self.trained_models[f"{model_name}_{selector_name}"] = {
            'model': best_model,
            'training_type': "Bayesian Optimization",
            'hyperparameters': best_model.get_params(),
            'cv_result': best_result
        }
        logging.info(f"Bayesian Optimization Best Result for {model_name} with {selector_name}: {best_result}")
        print(f"Bayesian Optimization Best Result for {model_name} with {selector_name}: {best_result}")
=====./feature_selection/mi_feature_selector.py=====
from sklearn.feature_selection import mutual_info_classif
from sklearn.feature_selection import SelectKBest
from core.feature_selection.base_feature_selector import BaseFeatureSelector

class MutualInformationFeatureSelector(BaseFeatureSelector):
    def __init__(self, X_train, y_train, k=10):
        super().__init__(X_train, y_train, k=k)

    def _create_selector(self, k=10):
        selector = SelectKBest(mutual_info_classif, k=k)
        selector.fit(self.X_train, self.y_train)
        return selector

    def get_search_space(self):
        return {'feature_selection__k': [5, 10, 20, 30, 40, 50, 'all']}=====./feature_selection/base_feature_selector.py=====
from abc import ABC, abstractmethod

class BaseFeatureSelector(ABC):
    def __init__(self, X_train, y_train=None, **kwargs):
        self.X_train = X_train
        self.y_train = y_train
        self.selector = self._create_selector(**kwargs)
    
    @abstractmethod
    def _create_selector(self, **kwargs):
        """
        Método abstrato para criar o seletor de características.
        Deve ser implementado por todas as subclasses.
        """
        pass

    @abstractmethod
    def get_search_space(self):
        """
        Método abstrato para retornar o espaço de busca de hiperparâmetros.
        Deve ser implementado por todas as subclasses.
        """
        pass
=====./feature_selection/feature_selection_factory.py=====
import numpy as np

from core.feature_selection.pca_feature_selector import PCAFeatureSelector
from core.feature_selection.random_forest_feature_selector import RandomForestFeatureSelector
from core.feature_selection.rfe_feature_selector import RFEFeatureSelector
from core.feature_selection.mi_feature_selector import MutualInformationFeatureSelector

class FeatureSelectionFactory:
    @staticmethod
    def create_selector(method, X_train, y_train=None, **kwargs):
        if method == 'rfe':
            selector = RFEFeatureSelector(X_train, y_train, **kwargs)
        elif method == 'pca':
            selector = PCAFeatureSelector(X_train, **kwargs)
        elif method == 'rf':
            selector = RandomForestFeatureSelector(X_train, y_train)
        elif method == 'mi':
            selector = MutualInformationFeatureSelector(X_train, y_train, **kwargs)
        else:
            raise ValueError(f"Método desconhecido: {method}")
        return selector

    @staticmethod
    def get_available_selectors():
        """
        Retorna uma lista dos métodos de seleção de características disponíveis.
        """
        return ['rfe', 'pca', 'rf', 'mi']

    @staticmethod
    def extract_selected_features(pipeline, feature_names):
        """
        Extrai as características selecionadas pelo seletor de características no pipeline.

        Args:
            pipeline: Pipeline treinado.
            feature_names: Lista de nomes das características originais.

        Returns:
            List: Lista de características selecionadas.
        """
        selector = pipeline.named_steps['feature_selection']

        if hasattr(selector, 'get_support'):
            mask = selector.get_support()
            selected_features = np.array(feature_names)[mask]
        elif hasattr(selector, 'transform'):
            # Para métodos como PCA que transformam as características
            transformed = selector.transform(np.identity(len(feature_names)))
            # Retornar os componentes principais como nomes
            selected_features = [f'PC{i+1}' for i in range(transformed.shape[1])]
        else:
            raise ValueError("O seletor não tem métodos para extrair características.")

        return selected_features
=====./feature_selection/random_forest_feature_selector.py=====
import numpy as np
import logging
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel

from core.feature_selection.base_feature_selector import BaseFeatureSelector

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RandomForestFeatureSelector(BaseFeatureSelector):
    def __init__(self, X_train, y_train, max_features=None):
        self.max_features = max_features
        super().__init__(X_train, y_train)

    def _create_selector(self):
        n_features = self.X_train.shape[1]
        estimator = RandomForestClassifier(n_estimators=100, random_state=42)
        estimator.fit(self.X_train, self.y_train)
        
        # Se max_features não for especificado, use metade das features
        if self.max_features is None or self.max_features == 'auto':
            self.max_features = max(1, n_features // 2)
        elif isinstance(self.max_features, float):
            self.max_features = max(1, int(self.max_features * n_features))
        
        selector = SelectFromModel(estimator, max_features=self.max_features)
        
        selected_features = selector.get_support().sum()
        logger.info(f"Selected {selected_features} features")
        
        return selector

    def get_search_space(self):
        n_features = self.X_train.shape[1]
        max_features_range = list(range(1, n_features + 1))
        return {
            'feature_selection__max_features': max_features_range,
            'feature_selection__threshold': ['mean', 'median', '0.5*mean', '1.5*mean']
        }

    def set_params(self, **params):
        if 'max_features' in params:
            self.max_features = int(params['max_features'])
            self.selector.max_features = self.max_features
        
        if 'threshold' in params:
            if isinstance(params['threshold'], str):
                estimator = self.selector.estimator
                feature_importances = estimator.feature_importances_
                if params['threshold'] == 'mean':
                    threshold = np.mean(feature_importances)
                elif params['threshold'] == 'median':
                    threshold = np.median(feature_importances)
                elif params['threshold'] == '0.5*mean':
                    threshold = 0.5 * np.mean(feature_importances)
                elif params['threshold'] == '1.5*mean':
                    threshold = 1.5 * np.mean(feature_importances)
                self.selector.threshold = threshold
            else:
                self.selector.threshold = params['threshold']
        return self=====./feature_selection/rfe_feature_selector.py=====
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import RFE

from core.feature_selection.base_feature_selector import BaseFeatureSelector

class RFEFeatureSelector(BaseFeatureSelector):
    def __init__(self, X_train, y_train, n_features_to_select=10):
        super().__init__(X_train, y_train, n_features_to_select=n_features_to_select)

    def _create_selector(self, n_features_to_select=10):
        n_features = self.X_train.shape[1]
        estimator = RandomForestClassifier(n_estimators=100, random_state=42)
        selector = RFE(estimator, n_features_to_select=min(n_features_to_select, n_features))
        return selector

    def get_search_space(cls):
        return {'feature_selection__n_features_to_select': [1, 5, 10, 20, 30, 40, 50]}=====./feature_selection/pca_feature_selector.py=====
from sklearn.decomposition import PCA

from core.feature_selection.base_feature_selector import BaseFeatureSelector

class PCAFeatureSelector(BaseFeatureSelector):
    def __init__(self, X_train, n_components=5, step=1):
        super().__init__(X_train, n_components=n_components)
        self.step = step
        self.n_components = n_components

    def _create_selector(self, n_components=5):
        n_features = self.X_train.shape[1]
        selector = PCA(n_components=min(n_components, n_features))
        return selector

    def get_search_space(self):
        return {'feature_selection__n_components': list(range(1, self.n_components + 1, self.step))}=====./exploration/data_exploration.py=====
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
from typing import Union, List, Dict

class DataExploration:

    @staticmethod
    def has_few_classes(column: pd.Series, num_classes: int = 5) -> bool:
        return column.nunique() < num_classes

    @staticmethod
    def inspect_dataframe(df: Union[pd.DataFrame, pd.Series]) -> None:
        print('Primeiras linhas do DataFrame:')
        print(df.head())
        print('\nInformações sobre o DataFrame:')
        print(df.info())

    @staticmethod
    def inspect_numeric_data(df: pd.DataFrame) -> None:
        print('Estatísticas descritivas do DataFrame:')
        print(df.select_dtypes(include=['float64', 'int64']).describe())

    @staticmethod
    def inspect_categorical_data(df: Union[pd.DataFrame, pd.Series]) -> None:
        print('Número de instâncias por classe:')
        if isinstance(df, pd.Series):
            DataExploration._print_series_categories(df)
        else:
            for i, column in enumerate(df.columns, start=1):
                print(f"({i}) {column}:")
                DataExploration._print_series_categories(df[column])

    @staticmethod
    def _print_series_categories(series: pd.Series) -> None:
        if series.nunique() < 10:
            for category in series.unique():
                count = (series == category).sum()
                print(f"    Categoria: {category}, Contagem: {count}")
        else:
            print(f"    Número de categorias: {series.nunique()}")

    @staticmethod
    def create_metadata_file(df: pd.DataFrame, file_path: str = 'data/metadata.csv') -> None:
        metadata = df.dtypes
        metadata.to_csv(file_path, header=['data_type'], sep=';')
        print(f'Metadados salvos em {file_path}')

    @staticmethod
    def visualize_histogram(df: pd.DataFrame, num_bins: int = 10, x_range: tuple = (0, 0), y_range: tuple = (0, 0)) -> None:
        axes = df.hist(bins=num_bins, figsize=(20, 15))
        for ax in axes.flatten():
            if x_range != (0, 0):
                ax.set_xlim(x_range)
            if y_range != (0, 0):
                ax.set_ylim(y_range)
        plt.show()

    @staticmethod
    def visualize_correlation_numeric(df: pd.DataFrame) -> None:
        corr = df.corr()
        plt.figure(figsize=(12, 10))
        sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f")
        plt.title('Matriz de Correlação')
        plt.show()

    @staticmethod
    def visualize_correlation_categorical(X: pd.DataFrame, y: pd.DataFrame, output_dir: str = '../output/heatmaps', batch_size: int = 25) -> None:
        os.makedirs(output_dir, exist_ok=True)
        X_cat = X.select_dtypes(include=['object', 'category'])
        y_cat = y.select_dtypes(include=['object', 'category'])

        total_plots = len(X_cat.columns) * len(y_cat.columns)
        batches = (total_plots // batch_size) + (total_plots % batch_size != 0)

        for batch in range(batches):
            fig, axs = plt.subplots(min(batch_size, total_plots - batch * batch_size), 1, figsize=(5, min(batch_size, total_plots - batch * batch_size) * 5))
            axs = [axs] if total_plots - batch * batch_size == 1 else axs
            for k, (i, j) in enumerate([(i, j) for i in range(len(X_cat.columns)) for j in range(len(y_cat.columns))][batch * batch_size:(batch + 1) * batch_size]):
                col_x, col_y = X_cat.columns[i], y_cat.columns[j]
                contingency_table = pd.crosstab(index=X_cat[col_x], columns=y_cat[col_y])
                sns.heatmap(contingency_table, annot=True, cmap='coolwarm', fmt=".2f", ax=axs[k])
                axs[k].set_title(f'{col_x} e {col_y}')
            
            plt.tight_layout()
            plt.savefig(os.path.join(output_dir, f'heatmap_batch_{batch + 1}.png'))
            plt.close(fig)

    @staticmethod
    def visualize_feature_target_correlation(X: pd.DataFrame, y: pd.Series, top_n: int = 10, figsize: tuple = (12, 8)) -> None:
        """
        Visualiza a correlação entre as features numéricas de X e o target numérico y.

        Args:
            X (pd.DataFrame): DataFrame contendo as features.
            y (pd.Series): Series contendo o target numérico.
            top_n (int): Número de features com maior correlação absoluta a serem exibidas.
            figsize (tuple): Tamanho da figura para o plot.

        Returns:
            None
        """
        if not pd.api.types.is_numeric_dtype(y):
            raise ValueError("O target (y) deve ser numérico para calcular correlações.")

        # Selecionar apenas colunas numéricas de X
        X_numeric = X.select_dtypes(include=['float64', 'int64'])

        # Calcular correlações
        correlations = X_numeric.apply(lambda x: x.corr(y) if pd.api.types.is_numeric_dtype(x) else 0)

        # Ordenar correlações por valor absoluto e selecionar top_n
        top_correlations = correlations.abs().nlargest(top_n)

        # Criar um DataFrame com as correlações para facilitar o plotting
        corr_df = pd.DataFrame({'feature': top_correlations.index, 'correlation': correlations[top_correlations.index]})
        corr_df = corr_df.sort_values('correlation', key=abs, ascending=True)

        # Plotar
        plt.figure(figsize=figsize)
        ax = sns.barplot(x='correlation', y='feature', data=corr_df, orient='h')
        ax.axvline(x=0, color='black', linewidth=0.5)
        plt.title(f'Top {top_n} Correlações entre Features e Target')
        plt.xlabel('Correlação')
        plt.ylabel('Feature')
        
        # Adicionar valores de correlação nas barras
        for i, v in enumerate(corr_df['correlation']):
            ax.text(v, i, f'{v:.2f}', va='center', fontweight='bold')

        plt.tight_layout()
        plt.show()

    @staticmethod
    def get_feature_target_correlation(X: pd.DataFrame, y: pd.Series) -> pd.Series:
        """
        Calcula a correlação entre as features numéricas de X e o target numérico y.

        Args:
            X (pd.DataFrame): DataFrame contendo as features.
            y (pd.Series): Series contendo o target numérico.

        Returns:
            pd.Series: Series contendo as correlações ordenadas por valor absoluto.
        """
        if not pd.api.types.is_numeric_dtype(y):
            raise ValueError("O target (y) deve ser numérico para calcular correlações.")

        X_numeric = X.select_dtypes(include=['float64', 'int64'])
        correlations = X_numeric.apply(lambda x: x.corr(y) if pd.api.types.is_numeric_dtype(x) else 0)
        return correlations.sort_values(key=abs, ascending=False)

=====./evaluation/evaluation.py=====
import numpy as np
import pandas as pd
from core.feature_selection.feature_selection_factory import FeatureSelectionFactory

from sklearn.metrics import (
    average_precision_score,
    classification_report,
    balanced_accuracy_score,
    cohen_kappa_score,
    confusion_matrix
)

class Evaluation:
    @staticmethod
    def evaluate_all_models(trained_models, X_train, y_train, X_test, y_test, feature_names=None):
        class_metrics_results = {}
        avg_metrics_results = {}

        for model_name, model_info in trained_models.items():
            model = model_info['model']
            cv_score = model_info['cv_result']

            train_results = Evaluation._evaluate_model(model, X_train, y_train, feature_names)
            test_results = Evaluation._evaluate_model(model, X_test, y_test, feature_names)

            class_metrics_results[model_name] = {
                'train_class_report': train_results['class_report'],
                'train_conf_matrix': train_results['conf_matrix'],
                'test_class_report': test_results['class_report'],
                'test_conf_matrix': test_results['conf_matrix'],
                'selected_features': test_results['selected_features']
            }

            avg_metrics_results[model_name] = {
                'cv_report': cv_score,
                'train_avg_metrics': train_results['avg_metrics'],
                'test_avg_metrics': test_results['avg_metrics'],
                'training_type': model_info['training_type'],
                'hyperparameters': model_info['hyperparameters']
            }

        return class_metrics_results, avg_metrics_results

    @staticmethod
    def _evaluate_model(model, X, y, feature_names=None):
        y_pred = model.predict(X)
        y_prob = model.predict_proba(X)

        class_report = Evaluation._generate_class_report(y, y_pred)
        avg_metrics = Evaluation._generate_avg_metrics(y, y_pred, y_prob)
        conf_matrix = Evaluation._generate_conf_matrix(y, y_pred)
        selected_features = Evaluation._extract_selected_features(model, feature_names)

        return {
            'class_report': class_report,
            'avg_metrics': avg_metrics,
            'conf_matrix': conf_matrix,
            'selected_features': selected_features
        }

    @staticmethod
    def _generate_class_report(y_true, y_pred):
        class_report = classification_report(y_true, y_pred, output_dict=True)
        class_report_df = pd.DataFrame(class_report).transpose().round(2)
        return class_report_df.drop(['accuracy', 'macro avg', 'weighted avg'])

    @staticmethod
    def _generate_avg_metrics(y_true, y_pred, y_prob):
        avg_metrics = {
            'balanced_accuracy': {'f1-score': balanced_accuracy_score(y_true, y_pred)},
            'kappa': {'f1-score': cohen_kappa_score(y_true, y_pred)},
            'auc_pr_macro': {'f1-score': Evaluation._calculate_auc_pr(y_true, y_prob)}
        }
        avg_metrics_df = pd.DataFrame(avg_metrics).transpose().round(2)

        class_report = classification_report(y_true, y_pred, output_dict=True)
        class_report_df = pd.DataFrame(class_report).transpose().round(2)
        additional_metrics = class_report_df.loc[['accuracy', 'macro avg', 'weighted avg']]

        return pd.concat([avg_metrics_df, additional_metrics])

    @staticmethod
    def _generate_conf_matrix(y_true, y_pred):
        conf_matrix = confusion_matrix(y_true, y_pred)
        classes = sorted(set(y_true))
        return pd.DataFrame(
            conf_matrix,
            index=[f'Actual {cls}' for cls in classes],
            columns=[f'Predicted {cls}' for cls in classes]
        )

    @staticmethod
    def _extract_selected_features(model, feature_names):
        if feature_names is not None:
            return FeatureSelectionFactory.extract_selected_features(model, feature_names)
        return None

    @staticmethod
    def _calculate_auc_pr(y_true, y_prob):
        auc_pr = [
            average_precision_score(y_true == i, y_prob[:, i])
            for i in range(len(np.unique(y_true)))
        ]
        return np.mean(auc_pr)=====./logging/report_formatter.py=====
import pandas as pd

class ReportFormatter:
    @staticmethod
    def generate_text_report(class_metrics_reports, avg_metrics_reports):
        report_output = ""
        for model_name, model_info in class_metrics_reports.items():
            avg_info = avg_metrics_reports[model_name]
            report_output += ReportFormatter._format_model_report(model_name, model_info, avg_info)
        return report_output

    @staticmethod
    def _format_model_report(model_name, model_info, avg_info):
        report = (f"\nEvaluating {model_name} with {avg_info['training_type']}:\n"
                  f"Hyperparameters: {avg_info['hyperparameters']}\n")
        
        if 'selected_features' in model_info:
            report += f"\nSelected Features: {', '.join(str(feature) for feature in model_info['selected_features'])}\n"
        
        report += ReportFormatter._format_set_report("Train", model_info, avg_info)
        report += ReportFormatter._format_set_report("Test", model_info, avg_info)
        return report

    @staticmethod
    def _format_set_report(set_name, model_info, avg_info):
        report = f"\n{set_name} set class report:\n"
        report += ReportFormatter._format_classification_report(model_info[f'{set_name.lower()}_class_report'])
        report += f"\n{set_name} set average metrics:\n"
        report += ReportFormatter._format_classification_report(avg_info[f'{set_name.lower()}_avg_metrics'])
        report += f"\n{set_name} set confusion matrix:\n"
        report += model_info[f'{set_name.lower()}_conf_matrix'].to_string() + "\n"
        return report

    @staticmethod
    def _format_classification_report(report_df):
        output = ""
        for index, row in report_df.iterrows():
            if index in ['accuracy', 'balanced_accuracy']:
                output += f"{index.capitalize()}: {row['f1-score']}\n"
            else:
                output += (f"Class {index} - Precision: {row['precision']}, Recall: {row['recall']}, "
                           f"F1-Score: {row['f1-score']}, Support: {row['support']}\n")
        return output

    @staticmethod
    def generate_class_report_dataframe(class_metrics_reports):
        return pd.concat([
            ReportFormatter._prepare_model_report(model_name, model_info)
            for model_name, model_info in class_metrics_reports.items()
        ])

    @staticmethod
    def _prepare_model_report(model_name, model_info):
        train_report = model_info['train_class_report'].add_suffix('-train')
        test_report = model_info['test_class_report'].add_suffix('-test')
        combined_report = train_report.join(test_report)
        combined_report['Model'] = model_name
        return combined_report.reset_index().rename(columns={'index': 'Metric'})

    @staticmethod
    def generate_avg_metrics_report_dataframe(avg_metrics_reports):
        return pd.concat([
            ReportFormatter._prepare_avg_metrics_report(model_name, model_info)
            for model_name, model_info in avg_metrics_reports.items()
        ])

    @staticmethod
    def _prepare_avg_metrics_report(model_name, model_info):
        train_metrics = model_info['train_avg_metrics'].add_suffix('-train')
        test_metrics = model_info['test_avg_metrics'].add_suffix('-test')
        combined_report = train_metrics.join(test_metrics)
        combined_report['Model'] = model_name
        return combined_report.reset_index().rename(columns={'index': 'Metric'})=====./logging/logger_config.py=====
import logging
import os
from datetime import datetime
from pathlib import Path

class LoggerConfig:
    @staticmethod
    def configure_log_file(file_main_name='bayesian_optimization', log_extension=".log", logger_name=None):
        """
        Configura um arquivo de log. Pode configurar o logger raiz ou um logger nomeado.

        Args:
            file_main_name (str): Nome base para o arquivo de log.
            log_extension (str): Extensão do arquivo de log.
            logger_name (str, optional): Nome do logger a ser configurado. Se None, configura o logger raiz.
        """
        output_dir = LoggerConfig._get_output_directory()
        log_filename = LoggerConfig._generate_log_filename(file_main_name, log_extension)
        log_file_path = os.path.join(output_dir, log_filename)
        
        if logger_name:
            logger = logging.getLogger(logger_name)
            logger.setLevel(logging.DEBUG)  # Captura todos os níveis de log
            
            # Evita adicionar múltiplos handlers se já existirem
            if not logger.handlers:
                # Handler para arquivo com nível DEBUG
                fh = logging.FileHandler(log_file_path)
                fh.setLevel(logging.DEBUG)
                
                # Handler para console com nível INFO
                ch = logging.StreamHandler()
                ch.setLevel(logging.INFO)
                
                # Formatação dos logs
                formatter = logging.Formatter('%(asctime)s | %(name)s | %(levelname)s | %(message)s')
                fh.setFormatter(formatter)
                ch.setFormatter(formatter)
                
                # Adiciona os handlers ao logger
                logger.addHandler(fh)
                logger.addHandler(ch)
        else:
            # Configura o logger raiz se ainda não estiver configurado
            root_logger = logging.getLogger()
            if not root_logger.handlers:
                logging.basicConfig(
                    filename=log_file_path,
                    level=logging.INFO,
                    filemode='w',
                    format='%(asctime)s:%(levelname)s:%(message)s'
                )

    @staticmethod
    def _get_output_directory():
        """
        Obtém o diretório de saída para armazenar os arquivos de log.

        Returns:
            str: Caminho do diretório de saída.
        """
        # Obtém o diretório atual onde o arquivo Python está localizado
        current_dir = Path(__file__).resolve()
        
        # Encontra a raiz do projeto subindo até encontrar a pasta 'src'
        src_dir = current_dir
        while src_dir.name != 'behavior-detection' and src_dir.parent != src_dir:
            src_dir = src_dir.parent

        # Verifica se a pasta 'src' foi encontrada
        if src_dir.name != 'behavior-detection':
            raise FileNotFoundError("Diretório 'src' não encontrado na estrutura de diretórios.")
        
        # Define o diretório de saída dentro da pasta src
        output_dir = src_dir / 'output'
        os.makedirs(output_dir, exist_ok=True)
        return output_dir

    @staticmethod
    def _generate_log_filename(file_main_name, log_extension):
        """
        Gera um nome de arquivo para o log baseado no timestamp atual.

        Args:
            file_main_name (str): Nome base para o arquivo de log.
            log_extension (str): Extensão do arquivo de log.

        Returns:
            str: Nome completo do arquivo de log.
        """
        return datetime.now().strftime(f'{file_main_name}_%Y%m%d_%H%M{log_extension}')

    @staticmethod
    def log_results(result):
        """
        Registra os resultados de uma iteração de otimização.

        Args:
            result: Resultado da iteração (deve ter atributos x_iters e func_vals).
        """
        if hasattr(result, 'x_iters') and hasattr(result, 'func_vals') and result.x_iters:
            score = abs(result.func_vals[-1])  # Use valor absoluto para simplificar
            logging.info(f"Iteration {len(result.x_iters)}: tested parameters: {result.x_iters[-1]}, score: {score}")

def main():
    LoggerConfig.configure_log_file('example_log')
    logging.info('Log configuration successful.')

if __name__ == "__main__":
    main()
=====./logging/model_manager.py=====
import os
import joblib
from core.logging.file_utils import FileUtils

class ModelManager:
    @staticmethod
    def save_model(model, filename, directory=None):
        directory = FileUtils._create_directory_if_not_exists(directory)
        file_path = FileUtils._generate_filename_with_timestamp(filename)
        file_path = os.path.join(directory, file_path) if directory else file_path
        joblib.dump(model, file_path)
        return file_path

    @staticmethod
    def load_model(filename, directory=None):
        file_path = os.path.join(directory, filename) if directory else filename
        return joblib.load(file_path)

    @classmethod
    def save_all_models(cls, trained_models, directory, prefix='model'):
        saved_models = []
        for model_name, model_info in trained_models.items():
            filename = f"{prefix}_{model_name}.pkl"
            file_path = cls.save_model(model_info['model'], filename, directory)
            saved_models.append(file_path)
            print(f"Model '{model_name}' saved at: {file_path}")
        return saved_models=====./logging/file_utils.py=====
import os
from datetime import datetime
import pandas as pd

class FileUtils:
    @staticmethod
    def save_file(content, filename, directory=None, is_csv=False):
        directory = FileUtils._create_directory_if_not_exists(directory)
        file_path = os.path.join(directory, filename)
        
        if is_csv:
            content.to_csv(file_path, index=False)
        else:
            with open(file_path, 'w') as file:
                file.write(content)
        
        return file_path

    @staticmethod
    def save_file_with_timestamp(content, filename, directory=None, is_csv=False):
        filename_with_timestamp = FileUtils._generate_filename_with_timestamp(filename)
        return FileUtils.save_file(content, filename_with_timestamp, directory, is_csv)

    @staticmethod
    def _create_directory_if_not_exists(directory):
        if directory:
            os.makedirs(directory, exist_ok=True)
        return directory or ""

    @staticmethod
    def _generate_filename_with_timestamp(filename):
        timestamp = datetime.now().strftime("%Y%m%d_%H%M")
        name, ext = os.path.splitext(filename)
        return f"{name}_{timestamp}{ext}"