{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = \"/Users/patricia/Documents/code/python-code/behavior-detection/src\"\n",
    "os.chdir(path)  # Muda o diretório para o nível anterior (a raiz do projeto)\n",
    "print(os.getcwd())  # Verifique se agora está na raiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from behavior.data.behavior_data_loader import BehaviorDataLoader\n",
    "\n",
    "data_path = '../data/new_logs_labels.csv'\n",
    "\n",
    "data = BehaviorDataLoader.load_data(data_path, delimiter=';')\n",
    "print(data.shape)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.preprocessors.data_cleaner import DataCleaner\n",
    "\n",
    "print(\"Valores da coluna 'comportamento' antes da remoção:\", data['comportamento'].value_counts())\n",
    "\n",
    "# Remove instances where 'comportamento' is '?'\n",
    "data = DataCleaner.remove_instances_with_value(data, 'comportamento', '?')\n",
    "\n",
    "print(\"\\nValores da coluna 'comportamento' depois da remoção:\", data['comportamento'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Select a subset of the data only for testing purposes\n",
    "\n",
    "print(\"Tamanho do dataframe antes:\", data.shape)\n",
    "data, _ = train_test_split(data, test_size=0.8, stratify=data['comportamento'], random_state=42)\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "print(\"Tamanho do dataframe após:\", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "## Remove unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing columns related to IDs, emotions, personality and behaviors, because \n",
    "# we want to classify behaviors only by the students' interactions with the system\n",
    "columns_to_remove_ids = ['id_log', 'grupo', 'num_dia', 'num_log']\n",
    "columns_to_remove_emotions = [\n",
    "    'estado_afetivo', 'estado_engajamento_concentrado', \n",
    "    'estado_confusao', 'estado_frustracao', 'estado_tedio', 'estado_indefinido', \n",
    "    'ultimo_estado_afetivo', 'ultimo_engajamento_concentrado', 'ultimo_confusao', \n",
    "    'ultimo_frustracao', 'ultimo_tedio', 'ultimo_estado_indefinido'\n",
    "]\n",
    "columns_to_remove_personality = [\n",
    "    'traco_amabilidade_fator', 'traco_extrovercao_fator', 'traco_conscienciosidade_fator', \n",
    "    'traco_abertura_fator', 'traco_neuroticismo_fator', 'traco_amabilidade_cat', \n",
    "    'traco_extrovercao_cat', 'traco_conscienciosidade_cat', 'traco_abertura_cat', \n",
    "    'traco_neuroticismo_cat']\n",
    "\n",
    "columns_to_remove_behaviors = [\n",
    "    'comportamento_on_task', 'comportamento_on_task_conversation', 'comportamento_on_task_out',\n",
    "    'comportamento_off_task', 'comportamento_on_system', 'comportamento_indefinido',\n",
    "    'ultimo_comportamento', 'ultimo_comportamento_on_task', 'ultimo_comportamento_on_task_conversation',\n",
    "    'ultimo_comportamento_on_task_out', 'ultimo_comportamento_off_task', 'ultimo_comportamento_on_system',\n",
    "    'ultimo_comportamento_indefinido'\n",
    "]\n",
    "\n",
    "columns_to_remove = columns_to_remove_ids + \\\n",
    "        columns_to_remove_emotions + \\\n",
    "        columns_to_remove_personality + \\\n",
    "        columns_to_remove_behaviors\n",
    "\n",
    "cleaned_data = DataCleaner.remove_columns(data, columns_to_remove)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preenche valores ausentes no DataFrame X com a string 'missing'.\n",
    "\n",
    "cleaned_data = cleaned_data.fillna('missing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data by student level into training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.preprocessors.data_splitter import DataSplitter\n",
    "\n",
    "train_data, test_data = DataSplitter.split_by_student_level(cleaned_data, test_size=0.2, column_name='aluno')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Após o split por nível do estudante\n",
    "print(\"\\n=== Após split por nível do estudante ===\")\n",
    "print(f\"Shape de train_data: {train_data.shape}\")\n",
    "print(\"Colunas em train_data:\", train_data.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the 'aluno' column from the data after splitting into train and test sets\n",
    "\n",
    "# Remover 'aluno' do conjunto de treinamento\n",
    "train_data = DataCleaner.remove_columns(train_data, ['aluno'])\n",
    "\n",
    "# Remover 'aluno' do conjunto de teste\n",
    "test_data = DataCleaner.remove_columns(test_data, ['aluno'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Após remover coluna 'aluno'\n",
    "print(\"\\n1. Após remover 'aluno':\")\n",
    "print(f\"Shape de train_data: {train_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into Features (X) and Target (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.preprocessors.data_splitter import DataSplitter\n",
    "\n",
    "# Conjunto de treinamento\n",
    "X_train, y_train = DataSplitter.split_into_x_y(train_data, 'comportamento')\n",
    "\n",
    "# Conjunto de teste\n",
    "X_test, y_test = DataSplitter.split_into_x_y(test_data, 'comportamento')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# 2. Após split X/y\n",
    "print(\"\\n2. Após split X/y:\")\n",
    "if isinstance(X_train, pd.DataFrame):\n",
    "    print(f\"Shape de X_train: {X_train.shape}\")\n",
    "    print(\"Primeiras colunas de X_train:\", list(X_train.columns)[:5])\n",
    "else:\n",
    "    print(\"X_train não é um DataFrame!\")\n",
    "    print(f\"Tipo de X_train: {type(X_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Primeiras 5 instâncias de y_train:\")\n",
    "print(y_train[:5])\n",
    "\n",
    "print(\"\\nPrimeiras 5 instâncias de y_test:\")\n",
    "print(y_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding true labels (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from core.preprocessors import column_selector, data_encoder\n",
    "from behavior.data import behavior_data_encoder\n",
    "\n",
    "# Recarregar o módulo para garantir que as alterações sejam aplicadas\n",
    "importlib.reload(column_selector)\n",
    "importlib.reload(data_encoder)\n",
    "importlib.reload(behavior_data_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding y_train and y_test\n",
    "from behavior.data.behavior_data_encoder import BehaviorDataEncoder\n",
    "\n",
    "# Codificar y_train\n",
    "y_train = BehaviorDataEncoder.encode_y(y_train)\n",
    "\n",
    "# Codificar y_test\n",
    "y_test = BehaviorDataEncoder.encode_y(y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding features (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importações necessárias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from behavior.data.behavior_data_encoder import BehaviorDataEncoder\n",
    "\n",
    "# Encoding do target (y)\n",
    "y_train = BehaviorDataEncoder.encode_y(y_train)\n",
    "y_test = BehaviorDataEncoder.encode_y(y_test)\n",
    "\n",
    "# Encoding das features (X)\n",
    "print(\"=== Iniciando encoding das features ===\")\n",
    "X_encoder = BehaviorDataEncoder(num_classes=5)\n",
    "print(\"\\nRealizando fit do encoder...\")\n",
    "X_encoder.fit(X_train)\n",
    "\n",
    "print(\"\\nRealizando transform...\")\n",
    "X_train = X_encoder.transform(X_train)\n",
    "\n",
    "print(\"\\nTransformando dados de teste...\")\n",
    "X_test = X_encoder.transform(X_test)\n",
    "\n",
    "# Verificação final\n",
    "print(\"\\n=== Verificação após encoding ===\")\n",
    "print(f\"Shape de X_train: {X_train.shape}\")\n",
    "print(f\"Shape de X_test: {X_test.shape}\")\n",
    "print(f\"Shape de y_train: {y_train.shape}\")\n",
    "print(f\"Shape de y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.shape)\n",
    "print(X_test.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Antes do SMOTE, adicione estas verificações\n",
    "print(\"Verificando X_train antes do SMOTE:\")\n",
    "print(\"1. Shape de X_train:\", X_train.shape)\n",
    "print(\"2. Tipo de X_train:\", type(X_train))\n",
    "print(\"3. Shape de y_train:\", y_train.shape)\n",
    "print(\"4. Tipo de y_train:\", type(y_train))\n",
    "\n",
    "if isinstance(X_train, pd.DataFrame):\n",
    "    print(\"5. Colunas em X_train:\")\n",
    "    print(X_train.columns.tolist())\n",
    "    print(\"\\n6. Primeiras linhas de X_train:\")\n",
    "    print(X_train.head())\n",
    "    print(\"\\n7. Tipos de dados das colunas:\")\n",
    "    print(X_train.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balanceamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.preprocessors.data_balancer import DataBalancer\n",
    "\n",
    "data_balancer = DataBalancer()\n",
    "X_train, y_train = data_balancer.apply_smote(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(f\"Resampled dataset shape: {Counter(y_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento dos Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definindo parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importações e configuração de diretório permanecem iguais até a seção de treinamento\n",
    "\n",
    "# Na seção \"Definindo parametros\", substituir:\n",
    "from core.models.multiclass.behavior_model_params import BehaviorModelParams\n",
    "\n",
    "# Criar instância dos parâmetros específicos para comportamentos\n",
    "model_params = BehaviorModelParams()\n",
    "\n",
    "# # Definir quais modelos e seletores utilizar\n",
    "# selected_models = [ \n",
    "#     # 'Logistic Regression',\n",
    "#     'Decision Tree',\n",
    "#     # 'Random Forest',\n",
    "#     # 'Gradient Boosting',\n",
    "#     # 'SVM',\n",
    "#     # 'KNN',\n",
    "#     # 'XGBoost',\n",
    "#     'Naive Bayes' \n",
    "#     # 'MLP'  \n",
    "# ]\n",
    "\n",
    "# # Definir quais seletores de features utilizar\n",
    "# selected_selectors = [\n",
    "#     # 'rfe',      # Recursive Feature Elimination\n",
    "#     'pca',      # Principal Component Analysis\n",
    "#     # 'rf',       # Random Forest Feature Selector\n",
    "#     # 'mi',       # Mutual Information Feature Selector\n",
    "#     'none'      # Sem seleção de features\n",
    "# ]\n",
    "\n",
    "\n",
    "# # Usar todos os modelos disponíveis\n",
    "selected_models = model_params.get_available_models()  # ou lista específica\n",
    "\n",
    "# # Usar todos os seletores disponíveis\n",
    "selected_selectors = None  # None to use all selectors\n",
    "\n",
    "# Configurar validação cruzada estratificada\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Parâmetros de otimização\n",
    "n_iter = 50  # Reduzido para teste inicial\n",
    "n_jobs = 6  # MacBook Air M2 tem 8 núclos CPUs e 10 GPUs. Como uso sciktlearn, só posso usar CPUs. Teria que usar Pytorch ou TensorFlow para usar GPUs\n",
    "scoring_metric = 'balanced_accuracy'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando Otimização Bayesiana (Optuna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.training.optuna_bayesian_optimization_training import OptunaBayesianOptimizationTraining\n",
    "\n",
    "# Instanciar e executar o treinamento\n",
    "training = OptunaBayesianOptimizationTraining()\n",
    "trained_models = training.train_model(\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    model_params=model_params,  \n",
    "    selected_models=selected_models,\n",
    "    selected_selectors=selected_selectors,\n",
    "    n_iter=n_iter,\n",
    "    cv=cv,\n",
    "    scoring=scoring_metric,\n",
    "    n_jobs=n_jobs\n",
    ")\n",
    "\n",
    "# Exemplo de acesso aos modelos treinados\n",
    "for model_key, model_info in trained_models.items():\n",
    "    print(f\"Modelo: {model_key}\")\n",
    "    print(f\"Melhores Hiperparâmetros: {model_info['hyperparameters']}\")\n",
    "    print(f\"Resultado CV: {model_info['cv_result']}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliação e logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.reporting import metrics_reporter\n",
    "\n",
    "# Avaliação dos Modelos\n",
    "class_metrics_results, avg_metrics_results =  metrics_reporter.evaluate_models(trained_models, X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Geração dos Relatórios\n",
    "metrics_reporter.generate_reports(class_metrics_results, avg_metrics_results, filename_prefix=\"_Optuna_behavior_\")\n",
    "\n",
    "# Salvando os modelos em arquivos para recuperação\n",
    "metrics_reporter.save_models(trained_models, filename_prefix=\"_Optuna_behavior_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projetos_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
