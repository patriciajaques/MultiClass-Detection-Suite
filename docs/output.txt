
=== base_data_processor.py ===
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE
import pandas as pd

class BaseDataProcessor:

    def __init__(self):
        self.data = self.load_data()

    def load_data(self, file_path='../data/new_logs_labels.csv'):
        df = pd.read_csv(file_path, delimiter=';')
        return df
    
    def split_data(self, df, target_column):
        """
        Faz o split em X e y.
        """

        X = df.drop(columns=[target_column])
        y = df[target_column]
        return X, y

    def get_data_by_type(self, data, data_type='categorical', num_classes=5):
        if data_type == 'categorical':
            condition = lambda col: (self.data[col].dtype == 'object' or self.data[col].dtype == 'int64') and self.data[col].nunique() < num_classes
        else:
            condition = lambda col: self.data[col].dtype in ['float64', 'int64'] and self.data[col].nunique() >= num_classes
        
        selected_columns = [col for col in self.data.columns if condition(col)]
        selected_data = self.data[selected_columns].copy()
        if data_type == 'categorical':
            selected_data = selected_data.astype('category')
        
        return selected_data

    def encode_single_column(self, data):
        le = LabelEncoder()
        return le.fit_transform(data), le

    def encode_categorical_columns(self, num_classes=5):
        categorical_data = self.get_data_by_type(data_type='categorical', num_classes=num_classes)
        X_encoded = self.data.copy()
        label_encoders = {}
        
        for col in categorical_data.columns:
            X_encoded[col], le = self.encode_single_column(X_encoded[col])
            label_encoders[col] = le
        
        return X_encoded, label_encoders

    def apply_encoders_to_test_data(self, X_test, label_encoders):
        X_test_encoded = X_test.copy()
        for col, le in label_encoders.items():
            if col in X_test_encoded.columns:
                X_test_encoded[col] = le.transform(X_test_encoded[col])
        return X_test_encoded
    
    def create_preprocessor(self):
        numeric_features = self.data.select_dtypes(include=['int64', 'float64']).columns
        categorical_features = self.data.select_dtypes(include=['object', 'category']).columns
        preprocessor = ColumnTransformer(transformers=[
            ('num', Pipeline([('scaler', MinMaxScaler())]), numeric_features),
            ('cat', Pipeline([('onehot', OneHotEncoder(handle_unknown='ignore'))]), categorical_features)
        ])
        return preprocessor

    def apply_smote(self, X_train, y_train):
        smote = SMOTE(random_state=42)
        return smote.fit_resample(X_train, y_train)

=== base_manager.py ===
from datetime import datetime
import os

class BaseManager:
    """
    Classe base responsável por operações comuns de arquivos.
    """

    @staticmethod
    def _generate_filename_with_timestamp(filename="report.txt"):
        """
        Gera um nome de arquivo com timestamp.
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M")
        name, ext = os.path.splitext(filename)
        return f"{name}_{timestamp}{ext}"

    @staticmethod
    def _create_directory_if_not_exists(directory):
        """
        Cria o diretório se ele não existir.
        """
        if directory:
            os.makedirs(directory, exist_ok=True)
            return directory
        return ""
=== bayesian_optimization_training.py ===
from skopt import BayesSearchCV
from model_training import ModelTraining
from model_params import get_bayes_search_spaces
from feature_selection import FeatureSelection
from logger_config import LoggerConfig
import logging

class BayesianOptimizationTraining(ModelTraining):
    """
    Subclasse específica para treinamento usando otimização bayesiana.
    """
    def __init__(self):
        # Configuração do logger no construtor
        super().__init__() 
        LoggerConfig.configure_log_file('bayesian_optimization', '.log')

    def optimize_model(self, pipeline, model_name, selector_name, X_train, y_train, n_iter, cv, scoring, n_jobs=-1):

        logging.info(f"Training and evaluating {model_name} with BayesianOptimization and {selector_name}:")
        print(f"Training and evaluating {model_name} with BayesianOptimization and {selector_name}:")

        # Bayesian Optimization
        search_space = get_bayes_search_spaces()[model_name]
        logging.info(f"Running Bayesian optimization for {model_name} with selector {selector_name}")
        logging.info(f"Search space: {search_space}")

        # Adicionar parâmetros para o seletor ao espaço de busca
        selector_search_space = FeatureSelection.get_search_spaces().get(selector_name, {})
        
        if isinstance(search_space, list):
            # Caso search_space seja uma lista, iterar e adicionar os parâmetros do seletor a cada subespaço
            for subspace in search_space:
                subspace.update(selector_search_space)
        else:
            # Caso search_space seja um dicionário, simplesmente atualizar
            search_space.update(selector_search_space)
        
        # Best model será um objeto do tipo Pipeline
        best_model, best_result, opt = self.execute_bayesian_optimization(pipeline, search_space, X_train, y_train, n_iter=n_iter, cv=cv, scoring=scoring, n_jobs=n_jobs)
        # Extraindo as predições de validação cruzada diretamente de cv_results_
        logging.info(f"Bayesian optimization results: Melhor resultado: {best_result}, Resultado médio da validação cruzada: {opt.cv_results_['mean_test_score'][opt.best_index_]}")

        # Armazenando mais informações sobre a configuração
        self.trained_models[f"{model_name}_{selector_name}"] = {
            'model': best_model,
            'training_type': "Bayesian Optimization",
            'hyperparameters': best_model.get_params(),  # Pegando hiperparâmetros do modelo
            'cv_result': best_result
        }
        logging.info(f"BayesianOptimization Best Result for {model_name} with {selector_name}: {best_result}")
        print(f"BayesianOptimization Best Result for {model_name} with {selector_name}: {best_result}")


    def execute_bayesian_optimization(self, pipeline, space, X_train, y_train, n_iter=50, cv=5, scoring='balanced_accuracy', n_jobs=-1):
        search = BayesSearchCV(
            pipeline,
            search_spaces=space,
            n_iter=n_iter,
            cv=cv,
            scoring=scoring,
            n_jobs=n_jobs,
            random_state=42,
            verbose=3
        )

        search.fit(X_train, y_train, callback=LoggerConfig.log_results)
        best_model = search.best_estimator_

        return best_model, search.best_score_, search



=== behaviors_data_processor.py ===
from base_data_processor import BaseDataProcessor
import utils
import pandas as pd

class BehaviorsDataProcessor(BaseDataProcessor):
    def __init__(self, data: pd.DataFrame):
        super().__init__(data)

    def load_data(self, file_path='../data/new_logs_labels.csv'):
        df = pd.read_csv(file_path, delimiter=';').query("comportamento != '?'")
        X, y = utils.split_features_and_target(df)
        return X, y['comportamento']

    def split_train_test_data(self, X, y, test_size=0.3, random_state=42):
        from data_exploration import concat_features_and_target 
        data = concat_features_and_target(X, y)
        train_data, test_data = utils.split_data_stratified(data, test_size=test_size, target_column='aluno', n_splits=5, random_state=random_state)
        X_train = train_data.drop(columns=['comportamento'])
        y_train = train_data['comportamento']
        X_test = test_data.drop(columns=['comportamento'])
        y_test = test_data['comportamento']
        return X_train, X_test, y_train.values, y_test.values
=== data_exploration.py ===
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
import preprocessing as pp

def load_data(file_path = '../data/new_logs_labels.csv'):
    """
    Lê um arquivo CSV com delimitador ';' e inspeciona seu conteúdo.

    Args:
        file_path (str): Caminho para o arquivo CSV.
    
    Returns:
        pd.DataFrame: DataFrame contendo os dados lidos.
    """
    
    # Ler o arquivo CSV com o delimitador ';'
    df = pd.read_csv(file_path, delimiter=';')

    return df

# Função para verificar se uma coluna possui menos de 5 classes
def has_few_classes(column, num_classes=5):
    return column.nunique() < num_classes

def inspect_df (df):
    """
    Inspeciona o DataFrame.

    Args:
        df (pd.DataFrame): DataFrame a ser inspecionado.
    """
    
    # Exibir as primeiras linhas do DataFrame
    print('Primeiras linhas do DataFrame:')
    print(df.head())
    print('\n')

    # Exibir informações sobre o DataFrame
    print('Informações sobre o DataFrame:')
    print(df.info())
    print('\n')

def inspect_num_data (df):
        # Exibir estatísticas descritivas do DataFrame
    print('Estatísticas descritivas do DataFrame:')
    print(df.select_dtypes(include=['float64']).describe())
    print('\n')


def inspect_cat_data (df):
    # Exibir o número de instancias para cada classe
    print('Nro de instancias por classe:')
    
    if isinstance(df, pd.Series):
        if df.nunique() < 10:
            for category in df.unique():
                count = (df == category).sum()
                print(f"Categoria: {category}, Contagem: {count}")
        else:
            print(f"Número de categorias: {df.nunique()}")
    else:
        for i, column in enumerate(df.columns, start=1):
            if df[column].nunique() < 5:
                print(f"({i}) {column}:")
                for category in df[column].unique():
                    count = (df[column] == category).sum()
                    print(f"    Categoria: {category}, Contagem: {count}")
            else:
                print(f"({i}) {column}: Número de categorias: {df[column].nunique()}")
    print('\n')

def create_metadata_file (df, file_path='data/metadata.csv'):

    """
    Cria um arquivo CSV contendo metadados do DataFrame.

    Args:
        df (pd.DataFrame): DataFrame a ser inspecionado.
    """
    
    # Extrair metadados do DataFrame
    metadata = df.dtypes

    # Salvar metadados em um arquivo CSV
    metadata.to_csv(file_path, header=['data_type'], sep=';')

    print('Metadados salvos em data/metadata.csv')

def concat_features_and_target(X, y):
    """
    Concatena as features (X) e o target (y) em um único DataFrame.

    Args:
        X (pd.DataFrame): DataFrame contendo as features.
        y (pd.DataFrame): DataFrame contendo o target.
    
    Returns:
        pd.DataFrame: DataFrame contendo as features e o target.
    """
    
    # Concatenar as features e o target
    df = pd.concat([X, y], axis=1)
    return df

def vis_histogram(df, num_bins = 10, x_min = 0, x_max = 0, y_min = 0, y_max = 0):
    """
    Visualiza o histograma das colunas do DataFrame.

    Args:
        df (pd.DataFrame): DataFrame contendo os dados.
    """
    
    # Criar os subplots do histograma
    axes = df.hist(bins=num_bins, figsize=(20, 15))

    for ax in axes.flatten():
        if x_min != x_max:
            ax.set_xlim([x_min, x_max])
        if y_min != y_max:
            ax.set_ylim([y_min, y_max])

    plt.show()

def vis_corr_num(df) :
    """
    Visualiza a matriz de correlação do DataFrame.

    Args:
        df (pd.DataFrame): DataFrame contendo os dados.
    """
    
    # Calcular a matriz de correlação
    corr = df.corr()
    
    # Plotar a matriz de correlação
    import seaborn as sns
    import matplotlib.pyplot as plt

    plt.figure(figsize=(12, 10))
    sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f")
    plt.title('Matriz de Correlação')
    plt.show()

def vis_corr_cat(X, y, output_dir='../output/heatmaps', batch_size=25):
    # Criação do diretório de saída se não existir
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    X_cat = pp.get_categorical_data(X)
    y_cat = pp.get_categorical_data(y)

    n = len(X_cat.columns)
    m = len(y_cat.columns)

    total_plots = n * m
    batches = (total_plots // batch_size) + (total_plots % batch_size != 0)

    plot_count = 0
    for batch in range(batches):
        fig, axs = plt.subplots(min(batch_size, total_plots), 1, figsize=(5, min(batch_size, total_plots)*5))
        for k in range(batch_size):
            if plot_count >= total_plots:
                break
            i, j = divmod(plot_count, m)
            col_x = X_cat.columns[i]
            col_y = y_cat.columns[j]
            contingency_table = pd.crosstab(index=X_cat[col_x], columns=y_cat[col_y])
            sns.heatmap(contingency_table, annot=True, cmap='coolwarm', fmt=".2f", ax=axs[k % batch_size])
            axs[k % batch_size].set_title(f'{col_x} e {col_y}')
            plot_count += 1
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, f'heatmap_batch_{batch + 1}.png'))
        plt.close(fig)

# Exemplo de chamada da função
# vis_corr_cat(X, y, output_dir='heatmaps', batch_size=25)


=== evaluation.py ===
import numpy as np
import pandas as pd
from sklearn.metrics import average_precision_score, classification_report, balanced_accuracy_score, cohen_kappa_score, confusion_matrix, precision_recall_curve

from feature_selection import FeatureSelection

class Evaluation:


    @staticmethod
    def evaluate_all_models(trained_models, X_train, y_train, X_test, y_test, feature_names=None):
        """
        Gera relatórios de avaliação para todos os modelos nos conjuntos de treino e teste, utilizando os resultados da
        validação cruzada realizada pelo BayesSearchCV.

        Args:
            trained_models: Dicionário de modelos treinados com BayesSearchCV.
            X_train: Conjunto de características de treino.
            y_train: Conjunto de rótulos de treino.
            X_test: Conjunto de características de teste.
            y_test: Conjunto de rótulos de teste.
            feature_names: Lista de nomes das características (opcional).

        Returns:
            class_metrics_results: Dicionário com os relatórios de avaliação por classe.
            avg_metrics_results: Dicionário com os relatórios de métricas médias.
        """
        class_metrics_results = {}
        avg_metrics_results = {}

        for model_name, model_info in trained_models.items():
            model = model_info['model']

            # Acessar os resultados da validação cruzada do BayesSearchCV
            cv_score = model_info['cv_result']

            # Avaliar o modelo no conjunto de treinamento usando predict
            train_class_report, train_conf_matrix, train_selected_features, train_avg_metrics = Evaluation.evaluate_model(model, X_train, y_train, feature_names)

            # Avaliar o modelo no conjunto de teste
            test_class_report, test_conf_matrix, test_selected_features, test_avg_metrics = Evaluation.evaluate_model(model, X_test, y_test, feature_names)
            
            class_metrics_results[model_name] = {
                'train_class_report': train_class_report,
                'train_conf_matrix': train_conf_matrix,
                'test_class_report': test_class_report,
                'test_conf_matrix': test_conf_matrix,
                'selected_features': test_selected_features
            }
            
            avg_metrics_results[model_name] = {
                'cv_report': cv_score,
                'train_avg_metrics': train_avg_metrics,
                'test_avg_metrics': test_avg_metrics,
                'training_type': model_info['training_type'],
                'hyperparameters': model_info['hyperparameters']
            }

        return class_metrics_results, avg_metrics_results

    @staticmethod
    def evaluate_model(model, X, y, feature_names=None):
        """
        Avalia um modelo e retorna as previsões, as métricas de avaliação e as características selecionadas.

        Args:
            model: Modelo treinado.
            X: Conjunto de características.
            y: Conjunto de rótulos.
            feature_names: Lista de nomes das características (opcional).

        Returns:
            class_metrics_df: DataFrame contendo o relatório de classificação por classe.
            conf_matrix_df: DataFrame contendo a matriz de confusão.
            selected_features: Lista de características selecionadas (se aplicável).
            avg_metrics_df: DataFrame contendo as métricas médias.
        """
        y_pred = model.predict(X)
        y_prob = model.predict_proba(X)  # Supondo que o modelo possui o método predict_proba para obter probabilidades

        # Inicializa os relatórios
        class_report = classification_report(y, y_pred, output_dict=True)
        avg_metrics = {}

        # Adiciona balanced_accuracy ao relatório de métricas médias
        bal_acc = balanced_accuracy_score(y, y_pred)
        avg_metrics['balanced_accuracy'] = {'precision': None, 'recall': None, 'f1-score': bal_acc, 'support': None}
        
        # Cálculo do Kappa e adição ao relatório de métricas médias
        kappa = cohen_kappa_score(y, y_pred)
        avg_metrics['kappa'] = {'precision': None, 'recall': None, 'f1-score': kappa, 'support': None}
        
        # Cálculo do AUC-PR e adição ao relatório de métricas médias
        auc_pr_macro = Evaluation.calculate_auc_pr(y, y_prob)
        avg_metrics['auc_pr_macro'] = {'precision': None, 'recall': None, 'f1-score': auc_pr_macro, 'support': None}

        # Converte os relatórios em DataFrames
        class_report_df = pd.DataFrame(class_report).transpose().round(2)
        avg_metrics_df = pd.DataFrame(avg_metrics).transpose().round(2)

        # Adiciona accuracy, macro avg e weighted avg ao relatório de métricas médias
        avg_metrics_df = pd.concat([avg_metrics_df, class_report_df.loc[['accuracy', 'macro avg', 'weighted avg']]])

        # Remove as métricas médias do relatório por classe
        class_report_df = class_report_df.drop(['accuracy', 'macro avg', 'weighted avg'])

        # Criação da matriz de confusão
        conf_matrix = confusion_matrix(y, y_pred)
        conf_matrix_df = pd.DataFrame(conf_matrix, 
                                      index=[f'Actual {cls}' for cls in sorted(set(y))], 
                                      columns=[f'Predicted {cls}' for cls in sorted(set(y))])

        # Extração das características selecionadas, se aplicável
        if feature_names is not None:
            selected_features = FeatureSelection.extract_selected_features(model, feature_names)
        else:
            selected_features = None

        return class_report_df, conf_matrix_df, selected_features, avg_metrics_df


    @staticmethod
    def calculate_auc_pr(y_true, y_prob):
        """
        # Calcula a AUC-PR macro iterando sobre cada classe, tratando-a como positiva e as demais como negativas, e calculando a média das AUC-PRs.
        Args:
            y_true (array-like): Classes verdadeiras.
            y_prob (array-like): Probabilidades previstas pelo modelo.

        Returns:
            float: AUC-PR média (macro) para todas as classes.
        """
        
        # Cálculo do AUC-PR para cada classe
        auc_pr = []
        for i in range(len(np.unique(y_true))):
            auc_pr.append(average_precision_score(y_true == i, y_prob[:, i]))
        
        auc_pr_macro = np.mean(auc_pr)
        
        return auc_pr_macro



=== feature_selection.py ===
import numpy as np
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import RFE, SelectFromModel

class FeatureSelection:

    @staticmethod
    def create_selectors(X_train, y_train):
        n_features = X_train.shape[1]
        selectors = {
            'rfe': FeatureSelection.create_selector('rfe', X_train=X_train, y_train=y_train, n_features_to_select=min(10, n_features)),
            'pca': FeatureSelection.create_selector('pca', X_train=X_train, n_components=min(5, n_features)),
            #'rf': create_selector('rf', X_train=X_train, y_train=y_train)
        }
        return selectors

    @staticmethod
    def create_selector(method, X_train=None, y_train=None, n_features_to_select=10, n_components=5):
        n_features = X_train.shape[1] if X_train is not None else None
        
        if method == 'rfe':
            estimator = RandomForestClassifier(n_estimators=100, random_state=42)
            selector = RFE(estimator, n_features_to_select=min(n_features_to_select, n_features))
        elif method == 'pca':
            selector = PCA(n_components=min(n_components, n_features))
        elif method == 'rf':
            estimator = RandomForestClassifier(n_estimators=100, random_state=0)
            estimator.fit(X_train, y_train)
            selector = SelectFromModel(estimator)
            selector.fit(X_train, y_train)
            
            initial_features = selector.get_support().sum()
            print(f"Inicialmente selecionadas {initial_features} características.")
            
            if initial_features == 0:
                selector.threshold_ = np.percentile(selector.estimator_.feature_importances_, 75)
                print(f"Ajuste do limiar para o percentil 75, novas características selecionadas: {selector.get_support().sum()}")

                if selector.get_support().sum() == 0:
                    selector.threshold_ = np.percentile(selector.estimator_.feature_importances_, 50)
                    print(f"Ajuste do limiar para o percentil 50, novas características selecionadas: {selector.get_support().sum()}")

                if selector.get_support().sum() == 0:
                    selector.threshold_ = np.percentile(selector.estimator_.feature_importances_, 25)
                    print(f"Ajuste do limiar para o percentil 25, novas características selecionadas: {selector.get_support().sum()}")
            
            final_features = selector.get_support().sum()
            print(f"Finalmente selecionadas {final_features} características.")
            
        else:
            raise ValueError(f"Unknown method: {method}")
        
        return selector

    @staticmethod
    def get_search_spaces():
        return {
            'rfe': {
                'feature_selection__n_features_to_select': [1, 5, 10, 20, 30, 40, 50]
            },
            'pca': {
                'feature_selection__n_components': [1, 5, 10, 20, 30, 40, 50]
            },
            'rf': {
                'feature_selection__threshold': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
            }
        }

    @staticmethod
    def extract_selected_features(pipeline, feature_names):
        """
        Extrai as características selecionadas pelo seletor de características no pipeline.
        
        Args:
            pipeline: Pipeline treinado.
            feature_names: Lista de nomes das características originais.

        Returns:
            List: Lista de características selecionadas.
        """
        selector = pipeline.named_steps['feature_selection']
        
        if hasattr(selector, 'get_support'):
            mask = selector.get_support()
            selected_features = np.array(feature_names)[mask]
        elif hasattr(selector, 'transform'):
            # Para métodos como PCA que não suportam diretamente 'get_support'
            selected_features = selector.transform(np.arange(len(feature_names)).reshape(1, -1)).flatten()
        else:
            raise ValueError("O seletor não tem métodos para extrair características.")
        
        return selected_features

=== file_manager.py ===
import os
from base_manager import BaseManager

class FileManager(BaseManager):
    """
    Classe responsável por operações de arquivos, como salvar textos e CSVs.
    """

    @staticmethod
    def save_text_file_with_timestamp(content, filename, directory=None):
        """
        Salva o conteúdo em um arquivo de texto com timestamp no nome do arquivo.
        """
        filename_with_timestamp = FileManager._generate_filename_with_timestamp(filename)
        return FileManager.save_text_file(content, filename_with_timestamp, directory)

    @staticmethod
    def save_csv_file_with_timestamp(dataframe, filename, directory=None):
        """
        Salva o DataFrame em um arquivo CSV com timestamp no nome do arquivo.
        """
        filename_with_timestamp = FileManager._generate_filename_with_timestamp(filename)
        return FileManager.save_csv_file(dataframe, filename_with_timestamp, directory)

    @staticmethod
    def save_text_file(content, filename, directory=None):
        """
        Salva o conteúdo em um arquivo de texto.
        
        Returns:
            file_path: Caminho completo do arquivo salvo.
        """
        directory = FileManager._create_directory_if_not_exists(directory)
        file_path = os.path.join(directory, filename)

        with open(file_path, 'w') as file:
            file.write(content)
        
        return file_path

    @staticmethod
    def save_csv_file(dataframe, filename, directory=None):
        """
        Salva o DataFrame em um arquivo CSV.
        
        Returns:
            file_path: Caminho completo do arquivo salvo.
        """
        directory = FileManager._create_directory_if_not_exists(directory)
        file_path = os.path.join(directory, filename)
        
        dataframe.to_csv(file_path, index=False)
        return file_path

=== logger_config.py ===
import logging
import os
from datetime import datetime

class LoggerConfig:
    @staticmethod
    def configure_log_file(file_main_name='bayesian_optimization', log_term=".log"):
        # Obter o caminho do diretório atual do script
        current_dir = os.path.dirname(__file__)
        
        # Construir o caminho até a pasta `output` no mesmo nível do diretório do script
        output_dir = os.path.join(current_dir, '..', 'output')
        
        # Certificar-se de que a pasta `output` existe
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        # Gerar um nome de arquivo com data e hora
        log_filename = datetime.now().strftime(file_main_name+'_%Y%m%d_%H%M'+log_term)

        # Caminho completo para o arquivo de log
        log_file_path = os.path.join(output_dir, log_filename)

        # Configurar o logger para usar o arquivo de log especificado
        logging.basicConfig(filename=log_file_path, level=logging.INFO, filemode='w',
                            format='%(asctime)s:%(levelname)s:%(message)s')

    @staticmethod
    def log_results(result):
        """
        Registra os parâmetros testados e a pontuação para cada iteração.
        Inverte a pontuação se ela for negativa, apenas para exibição.
        """
        if len(result.x_iters) > 0:  # Verificar se há iterações para logar
            # Inverter o sinal da pontuação para exibição se ela for negativa
            score = -result.func_vals[-1] if result.func_vals[-1] < 0 else result.func_vals[-1]
            logging.info(f"Iteration {len(result.x_iters)}: tested parameters: {result.x_iters[-1]}, score: {score}")

def main():
    # Exemplo de uso
    LoggerConfig.configure_log_file('meu_arquivo.log')
    logging.info('Registro de log configurado com sucesso.')

if __name__ == "__main__":
    main()
=== model_manager.py ===
import os
import joblib
from base_manager import BaseManager

class ModelManager(BaseManager):
    """
    Classe responsável por operações com modelos treinados.
    """

    @staticmethod
    def dump_model(model, filename, directory=None):
        """
        Salva o modelo treinado em um arquivo.
        
        Returns:
            file_path: Caminho completo do arquivo salvo.
        """
        directory = ModelManager._create_directory_if_not_exists(directory)
        file_path = os.path.join(directory, filename)
        
        joblib.dump(model, file_path)
        return file_path

    @staticmethod
    def load_model(filename, directory=None):
        """
        Carrega um modelo salvo a partir de um arquivo.
        
        Returns:
            model: Modelo carregado.
        """
        file_path = os.path.join(directory, filename) if directory else filename
        
        model = joblib.load(file_path)
        return model

    @staticmethod
    def dump_all_models(trained_models, directory, prefix='model'):
        """
        Salva todos os modelos treinados em arquivos individuais com data e hora.
        
        Returns:
            saved_models: Lista de caminhos completos dos arquivos salvos.
        """
        saved_models = []

        for model_name, model_info in trained_models.items():
            model = model_info['model']
            filename = ModelManager._generate_filename_with_timestamp(f"{prefix}_{model_name}.pkl")
            file_path = ModelManager.dump_model(model, filename, directory)
            saved_models.append(file_path)
            print(f"Modelo '{model_name}' salvo em: {file_path}")
        
        return saved_models
=== model_params.py ===
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
import xgboost as xgb

from skopt.space import Real, Integer, Categorical


def get_models():
    return {
        'Logistic Regression': LogisticRegression(max_iter=5000),
        'Decision Tree': DecisionTreeClassifier(),
        'Random Forest': RandomForestClassifier(),
        'Gradient Boosting': GradientBoostingClassifier(),
        'SVM': SVC(probability=True),
        'KNN': KNeighborsClassifier(),
        'XGBoost': xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
    }

def get_bayes_search_spaces():
    return {
        'Logistic Regression': [
            {
                'classifier__penalty': Categorical(['l2']),
                'classifier__C': Real(0.01, 10, prior='log-uniform'),
                'classifier__solver': Categorical(['lbfgs', 'saga']),
                'classifier__max_iter': Integer(1000, 10000)
            },
            {
                'classifier__penalty': Categorical(['l1']),
                'classifier__C': Real(0.01, 10, prior='log-uniform'),
                'classifier__solver': Categorical(['liblinear', 'saga']),
                'classifier__max_iter': Integer(1000, 10000)
            }
        ],
        'Decision Tree': {
            'classifier__max_depth': Categorical([None, 3, 5, 10, 20, 30]),
            'classifier__min_samples_split': Integer(2, 20),
            'classifier__min_samples_leaf': Integer(1, 10)
        },
        'Random Forest': {
            'classifier__n_estimators': Integer(50, 300),
            'classifier__max_depth': Integer(3, 30),
            'classifier__min_samples_split': Integer(2, 20),
            'classifier__min_samples_leaf': Integer(1, 10),
            'classifier__max_features': Categorical(['sqrt', 'log2', None])
        },
        'Gradient Boosting': {
            'classifier__n_estimators': Integer(50, 300),
            'classifier__learning_rate': Real(0.01, 0.2, prior='uniform'),
            'classifier__max_depth': Integer(3, 10),
            'classifier__subsample': Real(0.5, 1.0, prior='uniform'),
            'classifier__min_samples_split': Integer(2, 20),
            'classifier__min_samples_leaf': Integer(1, 10)
        },
        'SVM': [
            {
                'classifier__C': Real(0.01, 10, prior='log-uniform'),
                'classifier__kernel': Categorical(['rbf']),
                'classifier__gamma': Real(1e-4, 1e-1, prior='log-uniform')
            },
            {
                'classifier__C': Real(0.01, 10, prior='log-uniform'),
                'classifier__kernel': Categorical(['linear']),
                'classifier__gamma': Categorical(['scale'])  # `gamma` não se aplica ao kernel `linear`
            },
            {
                'classifier__C': Real(0.01, 10, prior='log-uniform'),
                'classifier__kernel': Categorical(['poly']),
                'classifier__gamma': Real(1e-4, 1e-1, prior='log-uniform'),
                'classifier__degree': Integer(2, 5)
            },
            {
                'classifier__C': Real(0.01, 10, prior='log-uniform'),
                'classifier__kernel': Categorical(['sigmoid']),
                'classifier__gamma': Real(1e-4, 1e-1, prior='log-uniform')
            }
        ],
        'KNN': {
            'classifier__n_neighbors': Integer(3, 20),
            'classifier__weights': Categorical(['uniform', 'distance']),
            'classifier__metric': Categorical(['euclidean', 'manhattan', 'minkowski'])
        },
        'XGBoost': {
            'classifier__n_estimators': Integer(50, 300),
            'classifier__learning_rate': Real(0.01, 0.2, prior='uniform'),
            'classifier__max_depth': Integer(3, 10),
            'classifier__subsample': Real(0.7, 1.0, prior='uniform'),
            'classifier__colsample_bytree': Real(0.7, 1.0, prior='uniform'),
            'classifier__reg_alpha': Real(0.0, 1.0, prior='uniform'),
            'classifier__reg_lambda': Real(0.0, 1.0, prior='uniform')
        }
    }


=== model_training.py ===
from abc import ABC, abstractmethod
from sklearn.pipeline import Pipeline

from model_params import get_models
from feature_selection import FeatureSelection as fs

class ModelTraining(ABC):
    """
    Superclasse abstrata para treinamento de modelos.
    """

    def __init__(self):
        self.trained_models = {}  # Inicializa o dicionário para armazenar os resultados de cada modelo

    def train_model(self, X_train, y_train, n_iter=50, cv=5, scoring='balanced_accuracy', n_jobs=-1):

        selectors = fs.create_selectors(X_train, y_train)  # Criar seletores

        models = get_models()

        for model_name, model_config in models.items():
            for selector_name, selector in selectors.items():
                
                # Criar pipeline
                pipeline = self.create_pipeline(selector, model_config)
                
                self.optimize_model(pipeline, model_name, selector_name, X_train, y_train, n_iter, cv, scoring, n_jobs)
        return self.trained_models
        
    @abstractmethod
    def optimize_model(self, pipeline, model_name, selector_name, X_train, y_train, n_iter, cv, scoring):
        pass

    def create_pipeline(self, selector, model_config):
        # Cria o pipeline diretamente com o seletor e o modelo
        pipeline = Pipeline([
            ('feature_selection', selector),
            ('classifier', model_config)
        ])
        return pipeline

=== preprocessing.py ===
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE
from IPython.core.debugger import set_trace


import utils
import pandas as pd

def get_data_by_type(data, data_type='categorical', num_classes=5):
    """
    Seleciona colunas do DataFrame baseado no tipo de dado e número de classes.
    
    Args:
        data (pd.DataFrame): DataFrame de entrada.
        data_type (str): Tipo de dado para seleção ('categorical' ou 'numerical').
        num_classes (int): Número de classes para considerar uma coluna categórica.
    
    Returns:
        pd.DataFrame: DataFrame com as colunas selecionadas.
    """
    if data_type == 'categorical':
        condition = lambda col: (data[col].dtype == 'object' or data[col].dtype == 'int64') and data[col].nunique() < num_classes
    else:
        condition = lambda col: data[col].dtype in ['float64', 'int64'] and data[col].nunique() >= num_classes
    
    selected_columns = [col for col in data.columns if condition(col)]
    selected_data = data[selected_columns].copy()
    if data_type == 'categorical':
        selected_data = selected_data.astype('category')
    
    return selected_data


def encode_single_column(data):
    """
    Aplica LabelEncoder a uma coluna ou série pandas.
    """
    le = LabelEncoder()
    return le.fit_transform(data), le

def encode_categorical_columns(X, num_classes=5):
    """
    Aplica LabelEncoder às variáveis categóricas de X e reutiliza encode_single_column.
    
    Args:
        X (pd.DataFrame): DataFrame de entrada.
        num_classes (int): Número de classes para considerar uma coluna categórica.
    
    Returns:
        pd.DataFrame: DataFrame com as colunas categóricas codificadas.
        dict: Dicionário de LabelEncoders usados para codificação.
    """
    # Identificar colunas categóricas usando get_data_by_type
    categorical_data = get_data_by_type(X, data_type='categorical', num_classes=num_classes)
    X_encoded = X.copy()
    label_encoders = {}
    
    for col in categorical_data.columns:
        X_encoded[col], le = encode_single_column(X_encoded[col])
        label_encoders[col] = le
    
    return X_encoded, label_encoders

def apply_encoders_to_test_data(X_test, label_encoders):
    """
    Aplica os LabelEncoders salvos de X_train em X_test.
    
    Parâmetros:
    - X_test: DataFrame contendo os dados de teste.
    - label_encoders: Dicionário contendo os LabelEncoders para cada coluna categórica.
    
    Retorna:
    - X_test_encoded: DataFrame com as colunas categóricas codificadas usando os LabelEncoders de X_train.
    """
    X_test_encoded = X_test.copy()
    for col, le in label_encoders.items():
        # Aplica o transform apenas nas colunas que existem em X_test e têm um LabelEncoder correspondente
        if col in X_test_encoded.columns:
            X_test_encoded[col] = le.transform(X_test_encoded[col])
    return X_test_encoded

def load_data(file_path='../data/new_logs_labels.csv'):
    """
    Lê, limpa e retorna os dados de um arquivo CSV.
    """
    df = pd.read_csv(file_path, delimiter=';').query("comportamento != '?'")
    X, y = utils.split_features_and_target(df)
    return X, y['comportamento']

def split_train_test_data(X, y, test_size=0.3, random_state=42):
    from data_exploration import concat_features_and_target 

    """
    Divide os dados em conjuntos de treino e teste.
    """
    data = concat_features_and_target(X, y)
    train_data, test_data = utils.split_data_stratified(data, test_size=test_size, target_column='aluno', n_splits=5, random_state=random_state)
    X_train = train_data.drop(columns=['comportamento'])
    y_train = train_data['comportamento']
    X_test = test_data.drop(columns=['comportamento'])
    y_test = test_data['comportamento']
    return X_train, X_test, y_train.values, y_test.values

def create_preprocessor(X_train):
    """
    Cria um pré-processador para colunas numéricas e categóricas.
    """
    numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns
    categorical_features = X_train.select_dtypes(include=['object', 'category']).columns
    preprocessor = ColumnTransformer(transformers=[
        ('num', Pipeline([('scaler', MinMaxScaler())]), numeric_features),
        ('cat', Pipeline([('onehot', OneHotEncoder(handle_unknown='ignore'))]), categorical_features)
    ])
    return preprocessor

def apply_smote(X_train, y_train):
    """
    Aplica SMOTE para realizar oversampling nos dados de treinamento.
    """
    smote = SMOTE(random_state=42)
    return smote.fit_resample(X_train, y_train)
=== report_formatter.py ===
import pandas as pd

class ReportFormatter:

    @staticmethod
    def generate_text_report_from_dict(class_metrics_reports, avg_metrics_reports):
        """
        Gera texto para relatórios detalhados de avaliação, incluindo matrizes de confusão.

        Args:
            class_metrics_reports: Dicionário com os relatórios de avaliação por classe.
            avg_metrics_reports: Dicionário com os relatórios de métricas médias.

        Returns:
            report_output: String com o conteúdo dos relatórios.
        """
        report_output = ""

        for model_name, model_info in class_metrics_reports.items():
            avg_info = avg_metrics_reports[model_name]
            report_output += (f"\nEvaluating {model_name} with {avg_info['training_type']}:\n"
                              f"Hyperparameters: {avg_info['hyperparameters']}\n")
            
            selected_features = model_info.get('selected_features', None)
            if selected_features is not None:
                report_output += f"\nSelected Features: {', '.join(str(feature) for feature in selected_features)}\n"
            
            # Relatório do conjunto de treino
            report_output += "\nTrain set class report:\n"
            report_output += ReportFormatter.format_report(model_info['train_class_report'])
            report_output += "\nTrain set average metrics:\n"
            report_output += ReportFormatter.format_report(avg_info['train_avg_metrics'])
            report_output += "\nTrain set confusion matrix:\n"
            report_output += model_info['train_conf_matrix'].to_string() + "\n"
            
            # Relatório do conjunto de teste
            report_output += "\nTest set class report:\n"
            report_output += ReportFormatter.format_report(model_info['test_class_report'])
            report_output += "\nTest set average metrics:\n"
            report_output += ReportFormatter.format_report(avg_info['test_avg_metrics'])
            report_output += "\nTest set confusion matrix:\n"
            report_output += model_info['test_conf_matrix'].to_string() + "\n"

        return report_output
    
    @staticmethod
    def format_report(report_df):
        """
        Formata o relatório de classificação como uma string.
        
        Args:
            report_df: DataFrame contendo o relatório de classificação.

        Returns:
            output: String formatada do relatório de classificação.
        """
        output = ""
        for index, row in report_df.iterrows():
            if index == 'accuracy':
                output += f"Accuracy - F1-Score: {row['f1-score']}\n"
            elif index == 'balanced_accuracy':
                output += f"Balanced Accuracy: {row['f1-score']}\n"
            else:
                output += (f"Class {index} - Precision: {row['precision']}, Recall: {row['recall']}, "
                           f"F1-Score: {row['f1-score']}, Support: {row['support']}\n")
        return output

    @staticmethod
    def generate_class_report_dataframe(class_metrics_reports):
        """
        Gera um DataFrame detalhado de classificação por classe.
        
        Args:
            class_metrics_reports: Dicionário com os relatórios de avaliação por classe.

        Returns:
            detailed_df: DataFrame contendo o relatório detalhado de classificação por classe.
        """
        all_class_info = []

        for model_name, model_info in class_metrics_reports.items():
            train_class_report = model_info['train_class_report'].add_suffix('-train')
            test_class_report = model_info['test_class_report'].add_suffix('-test')
            combined_report = train_class_report.join(test_class_report)
            
            combined_report['Model'] = model_name
            combined_report.reset_index(inplace=True)
            combined_report.rename(columns={'index': 'Metric'}, inplace=True)

            all_class_info.append(combined_report)

        detailed_df = pd.concat(all_class_info)
        return detailed_df

    @staticmethod
    def generate_avg_metrics_report_dataframe(avg_metrics_reports):
        """
        Gera um DataFrame resumido de métricas médias.
        
        Args:
            avg_metrics_reports: Dicionário com os relatórios de métricas médias.

        Returns:
            summary_df: DataFrame contendo o relatório resumido de métricas médias.
        """
        summary_model_info = []

        for model_name, model_info in avg_metrics_reports.items():
            train_avg_metrics = model_info['train_avg_metrics'].add_suffix('-train')
            test_avg_metrics = model_info['test_avg_metrics'].add_suffix('-test')
            combined_report = train_avg_metrics.join(test_avg_metrics)
            
            combined_report['Model'] = model_name
            combined_report.reset_index(inplace=True)
            combined_report.rename(columns={'index': 'Metric'}, inplace=True)

            summary_model_info.append(combined_report)

        summary_df = pd.concat(summary_model_info)
        return summary_df

=== utils.py ===
from sklearn.model_selection import train_test_split, StratifiedShuffleSplit

def determine_features_to_remove(df):
    """
    Retorna apenas as colunas que são features.

    Args:
        df (pd.DataFrame): DataFrame contendo os dados.
    
    Returns:
        pd.DataFrame: DataFrame contendo apenas as features.
    """
    
    # Selecionar apenas as colunas cujos nomes iniciam com 'traco_', 'estado_', 'comportamento_' e 'ultimo_'
    removed_features = df.loc[:, df.columns.str.startswith('traco_') | df.columns.str.startswith('estado_') | df.columns.str.startswith('comportamento') | df.columns.str.startswith('ultimo_')]
    removed_features = removed_features.drop('ultimo_passo_correto', axis=1)
    return removed_features

def get_personality_features(df):
    """
    Retorna apenas as colunas que são features de personalidade.

    Args:
        df (pd.DataFrame): DataFrame contendo os dados.
    
    Returns:
        pd.DataFrame: DataFrame contendo apenas as features de personalidade.
    """
    
    # Selecionar apenas as colunas cujos nomes iniciam com 'traco_'
    personality_features = df.loc[:, df.columns.str.startswith('traco_')]
    # Remover as colunas cujos nomes finalizam com '_cat''
    personality_features = personality_features.loc[:, ~personality_features.columns.str.endswith('_cat')]
    return personality_features

def get_behavior_features(df):
    """
    Retorna apenas as colunas que são features de comportamento.

    Args:
        df (pd.DataFrame): DataFrame contendo os dados.
    
    Returns:
        pd.DataFrame: DataFrame contendo apenas as features de comportamento.
    """
    
    # Selecionar apenas as colunas cujos nomes iniciam com 'comportamento_'
    #behavior_features = df.loc[:, df.columns.str.startswith('comportamento_') | df.columns.str.startswith('ultimo_comportamento_')]
    behavior_features = df.loc[:, df.columns.str.startswith('comportamento')]
    return behavior_features

def split_features_and_target(df):
    """
    Splits the DataFrame into features (X) and target (y).
    
    Parameters:
    - df: pd.DataFrame - The DataFrame containing features and target.
    
    Returns:
    - X: pd.DataFrame - containing the features.
    - y: pd.DataFrame - containing the target.
    """
    removed_features = determine_features_to_remove(df)
    X = df.drop(columns=removed_features.columns.tolist())
    y = get_behavior_features(removed_features)
    return X, y

def split_student_level(data, test_size=0.2, column_name = 'aluno'):
    """
    Splits the DataFrame into student level.
    
    Parameters:
    - df: pd.DataFrame - The DataFrame containing features and target.
    
    Returns:
    - X: pd.DataFrame - containing the features.
    - y: pd.DataFrame - containing the target.
    """
    # Identificar os IDs únicos dos estudantes
    unique_students = data[column_name].unique()
    num_total_students = len(unique_students)

    # Fazer a divisão dos estudantes em conjuntos de treino e teste
    train_students, test_students = train_test_split(unique_students, test_size, random_state=42)

    num_test_students = len(test_students)

    # Separar os dados com base nos IDs dos estudantes
    train_data = data[data[column_name].isin(train_students)]
    test_data = data[data[column_name].isin(test_students)]

    # Verificar o tamanho dos conjuntos
    print(f'Número total de alunos: {num_total_students}')
    print(f'Número de alunos no conjunto de teste: {num_test_students}')
    print(f'Tamanho do conjunto de treino: {len(train_data)}')
    print(f'Tamanho do conjunto de teste: {len(test_data)}')

    return train_data, test_data

def split_stratified_student_level(data, test_size=0.2, column_name='aluno', target_column='comportamento', n_splits=10):
    """
    Splits the DataFrame into student level and ensures a representative number of classes in the test set.
    
    Parameters:
    - data: pd.DataFrame - The DataFrame containing features and target.
    - test_size: float - Proportion of the dataset to include in the test split.
    - column_name: str - The column name identifying the students.
    - target_column: str - The target column to ensure class representation.
    
    Returns:
    - train_data: pd.DataFrame - Training data.
    - test_data: pd.DataFrame - Testing data.
    """
    # Identificar os IDs únicos dos estudantes
    unique_students = data[column_name].unique()
    num_total_students = len(unique_students)

    # Inicializar StratifiedShuffleSplit
    stratified_split = StratifiedShuffleSplit(n_splits, test_size=test_size, random_state=42)

    # Dividir os estudantes de forma estratificada
    for train_index, test_index in stratified_split.split(unique_students, data.groupby(column_name)[target_column].first().loc[unique_students]):
        train_students = unique_students[train_index]
        test_students = unique_students[test_index]

    num_test_students = len(test_students)

    # Separar os dados com base nos IDs dos estudantes
    train_data = data[data[column_name].isin(train_students)]
    test_data = data[data[column_name].isin(test_students)]

    # Verificar o tamanho dos conjuntos
    print(f'Número total de alunos: {num_total_students}')
    print(f'Número de alunos no conjunto de teste: {num_test_students}')
    print(f'Tamanho do conjunto de treino: {len(train_data)}')
    print(f'Tamanho do conjunto de teste: {len(test_data)}')

    return train_data, test_data

import pandas as pd
from sklearn.model_selection import train_test_split, StratifiedShuffleSplit

def split_data_stratified(data, test_size=0.2, target_column='comportamento', n_splits=10, random_state=42):
    """
    Splits the DataFrame into train and test sets ensuring a representative number of classes in the test set.
    
    Parameters:
    - data: pd.DataFrame - The DataFrame containing features and target.
    - test_size: float - Proportion of the dataset to include in the test split.
    - target_column: str - The target column to ensure class representation.
    
    Returns:
    - train_data: pd.DataFrame - Training data.
    - test_data: pd.DataFrame - Testing data.
    """
    # Inicializar StratifiedShuffleSplit
    stratified_split = StratifiedShuffleSplit(n_splits, test_size=test_size, random_state=random_state)

    # Dividir os dados de forma estratificada
    for train_index, test_index in stratified_split.split(data, data[target_column]):
        train_data = data.iloc[train_index]
        test_data = data.iloc[test_index]

    # Verificar o tamanho dos conjuntos
    print(f'Tamanho do conjunto de treino: {len(train_data)}')
    print(f'Tamanho do conjunto de teste: {len(test_data)}')

    return train_data, test_data


